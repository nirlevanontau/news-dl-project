{
  "cells": [
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9faf162c",
      "metadata": {
        "id": "9faf162c"
      },
      "source": [
        "## Import and preprocessing"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 3,
      "id": "89c53560",
      "metadata": {
        "id": "89c53560",
        "outputId": "c42842d4-766c-482e-c2b1-cc5128bc481c",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "[nltk_data] Downloading package wordnet to\n",
            "[nltk_data]     C:\\Users\\levan\\AppData\\Roaming\\nltk_data...\n",
            "[nltk_data]   Package wordnet is already up-to-date!\n",
            "c:\\Python310\\lib\\site-packages\\tqdm\\auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
            "  from .autonotebook import tqdm as notebook_tqdm\n"
          ]
        }
      ],
      "source": [
        "# imports for preprocessing\n",
        "import numpy as np\n",
        "import nltk\n",
        "import csv\n",
        "# importing the Stemming function from nltk library\n",
        "from nltk.stem.porter import PorterStemmer\n",
        "nltk.download('wordnet')\n",
        "from collections import Counter\n",
        "# from wordcloud import WordCloud\n",
        "from nltk.stem import WordNetLemmatizer\n",
        "from nltk.corpus import stopwords\n",
        "from string import punctuation\n",
        "from nltk.tokenize import WordPunctTokenizer\n",
        "import regex\n",
        "\n",
        "# imports for visualizations\n",
        "import matplotlib.pyplot as plt\n",
        "import seaborn as sns\n",
        "from transformers import BertTokenizer, BertModel, BertTokenizerFast\n",
        "from sklearn.decomposition import PCA\n",
        "from collections import defaultdict\n",
        "import wandb\n",
        "from prettytable import PrettyTable\n",
        "\n",
        "# imports for modeling\n",
        "import torch\n",
        "from transformers import Trainer, TrainingArguments\n",
        "import pandas as pd\n",
        "from sklearn.metrics import f1_score\n",
        "from transformers import BertForSequenceClassification, BertTokenizer, AutoConfig ,TFAutoModelForSequenceClassification\n",
        "from transformers import AutoTokenizer, AutoModelForSequenceClassification\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "from sklearn.model_selection import train_test_split\n",
        "from datasets import load_dataset\n",
        "import optuna\n",
        "import os\n",
        "from pathlib import Path\n",
        "\n",
        "\n",
        "# imports for pruning\n",
        "import copy\n",
        "import torch.nn.utils.prune as prune\n",
        "from torch import nn\n",
        "\n",
        "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 4,
      "id": "27c671e0",
      "metadata": {
        "id": "27c671e0"
      },
      "outputs": [],
      "source": [
        "home_dir = os.getcwd()\n",
        "output_dir = Path(home_dir, 'results')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 5,
      "id": "c2c00149",
      "metadata": {
        "id": "c2c00149",
        "outputId": "99894ae2-b088-4e9b-d1e8-be637852ae53"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "'c:\\\\Users\\\\levan\\\\Documents\\\\מסמכים של ניר\\\\אוניברסיטת תל אביב\\\\2022-2023\\\\נושאים מתקדמים בלמידה עמוקה\\\\פרויקט\\\\news-dl-project'"
            ]
          },
          "execution_count": 5,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "home_dir"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9d863c02",
      "metadata": {
        "id": "9d863c02"
      },
      "source": [
        "## Prepare the data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 6,
      "id": "ac1a7a50",
      "metadata": {
        "id": "ac1a7a50"
      },
      "outputs": [],
      "source": [
        "balanced_df = pd.read_csv('balanced_df.csv')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 7,
      "id": "7981a4d7",
      "metadata": {
        "id": "7981a4d7"
      },
      "outputs": [],
      "source": [
        "# if we run on cpu\n",
        "# balanced_df = balanced_df.iloc[:100]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 9,
      "id": "a5380cff",
      "metadata": {
        "id": "a5380cff"
      },
      "outputs": [],
      "source": [
        "headlines = balanced_df['text']\n",
        "categories = balanced_df['updated_category']\n",
        "\n",
        "# encode the labels to number\n",
        "label_encoder = LabelEncoder()\n",
        "balanced_df['label'] = label_encoder.fit_transform(categories.values)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 10,
      "id": "edd94839",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "4ebdc29db6ce4497b18f8499556cef49",
            "38f1869e6fc149f4bf8e8c933d6d6113",
            "",
            "5fd7f87b67ea409f9f40b8e5ab886fd5"
          ]
        },
        "id": "edd94839",
        "outputId": "53dec85f-d0e1-43fd-c774-d773f9f6e689"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading and preparing dataset csv/default to C:/Users/levan/.cache/huggingface/datasets/csv/default-7b22cbae0e9605fe/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d...\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Downloading data files: 100%|██████████| 2/2 [00:00<00:00, 1999.19it/s]\n",
            "Extracting data files: 100%|██████████| 2/2 [00:00<00:00, 40.00it/s]\n",
            "                                                                     \r"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Dataset csv downloaded and prepared to C:/Users/levan/.cache/huggingface/datasets/csv/default-7b22cbae0e9605fe/0.0.0/eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d. Subsequent calls will reuse this data.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "100%|██████████| 2/2 [00:00<00:00, 32.78it/s]\n"
          ]
        }
      ],
      "source": [
        "X = headlines.values\n",
        "y = balanced_df['label'].values\n",
        "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)\n",
        "\n",
        "train_df = pd.DataFrame({'headlines':X_train, 'labels':y_train})\n",
        "test_df = pd.DataFrame({'headlines':X_test, 'labels':y_test})\n",
        "\n",
        "train_df.to_csv('train_df.csv', index = False)\n",
        "test_df.to_csv('test_df.csv', index = False)\n",
        "\n",
        "data_files = {\n",
        "'train':'train_df.csv',\n",
        "'test':'test_df.csv'\n",
        "}\n",
        "\n",
        "datasets = load_dataset(\"csv\", data_files=data_files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 11,
      "id": "5923375b",
      "metadata": {
        "id": "5923375b",
        "outputId": "82f15607-9e01-48ed-9d88-66799678f9ba"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "DatasetDict({\n",
              "    train: Dataset({\n",
              "        features: ['headlines', 'labels'],\n",
              "        num_rows: 148771\n",
              "    })\n",
              "    test: Dataset({\n",
              "        features: ['headlines', 'labels'],\n",
              "        num_rows: 37193\n",
              "    })\n",
              "})"
            ]
          },
          "execution_count": 11,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "datasets"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "46a86778",
      "metadata": {
        "id": "46a86778"
      },
      "source": [
        "## Define the model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 12,
      "id": "d9b8020b",
      "metadata": {
        "id": "d9b8020b"
      },
      "outputs": [],
      "source": [
        "class Model():\n",
        "    def __init__(self, model_name, data_set, num_classes):\n",
        "        self.model_name=model_name\n",
        "        self.num_classes = num_classes\n",
        "        self.model= BertForSequenceClassification.from_pretrained(self.model_name, num_labels=self.num_classes, return_dict=True).to(device)\n",
        "        self.dataset=data_set\n",
        "\n",
        "\n",
        "    def tokenize(self, token_args):\n",
        "        self.tokenizer =  BertTokenizer.from_pretrained(self.model_name)\n",
        "        self.tokenized_dataset = self.dataset.map(self.tokenizer, input_columns='headlines', fn_kwargs=token_args)\n",
        "        self.tokenized_dataset.set_format('torch')\n",
        "\n",
        "    def metric_fn(self, predictions):\n",
        "        preds = predictions.predictions.argmax(axis=1)\n",
        "        labels = predictions.label_ids\n",
        "        return {'f1': f1_score(labels, preds, average='weighted')}\n",
        "\n",
        "\n",
        "    def train(self, train_args):\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=train_args,\n",
        "            train_dataset=self.tokenized_dataset['train'],\n",
        "            eval_dataset=self.tokenized_dataset['test'],\n",
        "            compute_metrics=self.metric_fn)\n",
        "\n",
        "        trainer.train()\n",
        "\n",
        "\n",
        "    def hyper_parameters_search(self, train_args):\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=train_args,\n",
        "            train_dataset=self.tokenized_dataset['train'],\n",
        "            eval_dataset=self.tokenized_dataset['test'],\n",
        "            model_init=self.model_init,\n",
        "            compute_metrics=self.metric_fn)\n",
        "\n",
        "        best_run = trainer.hyperparameter_search(direction=\"maximize\", hp_space=self.optuna_hp_space,n_trials=10)\n",
        "        chosen_hyperparameters = best_run.hyperparameters\n",
        "        wandb.finish()\n",
        "        print(f'{self.model_name} chosen hyperparameters:')\n",
        "        print(chosen_hyperparameters)\n",
        "\n",
        "\n",
        "    def model_init(self):\n",
        "        return self.model\n",
        "\n",
        "    def optuna_hp_space(self,trial):\n",
        "        return {\"learning_rate\": trial.suggest_float(\"learning_rate\", 1e-6, 1e-4, log=True),\n",
        "                \"num_train_epochs\": trial.suggest_categorical(\"num_train_epochs\", [7]),\n",
        "                \"seed\": trial.suggest_categorical(\"seed\", [9]),\n",
        "                \"per_device_train_batch_size\": trial.suggest_categorical(\"per_device_train_batch_size\", [16]),\n",
        "                \"gradient_accumulation_steps\":trial.suggest_int(\"gradient_accumulation_steps\",1,5),\n",
        "                \"warmup_steps\":trial.suggest_int(\"warmup_steps\",1,300),\n",
        "                \"weight_decay\":trial.suggest_float(\"weight_decay\",1e-3,1e-1),\n",
        "                \"per_device_eval_batch_size\":trial.suggest_categorical(\"per_device_eval_batch_size\",[16])}\n",
        "\n",
        "\n",
        "    def print_model(self):\n",
        "        x = PrettyTable()\n",
        "        x.field_names  = [\"Layer Name\", \"Sum of Weights\"]\n",
        "        total_sum = 0\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module, torch.nn.Linear):\n",
        "                x.add_row([name, round(module.weight.data.abs().sum().item(),3)])\n",
        "                total_sum+=module.weight.data.abs().sum().item()\n",
        "        x.add_row(['Total Sum of Weights', round(total_sum,3)])\n",
        "        print(x)\n",
        "\n",
        "    def print_model_size(self):\n",
        "        torch.save(self.model.state_dict(), Path(home_dir,\"tmp.pt\"))\n",
        "        print(\"%.2f MB\" %(os.path.getsize(Path(home_dir,\"tmp.pt\"))/1e6))\n",
        "\n",
        "    def pruning(self, amount):\n",
        "        for name, module in self.model.named_modules():\n",
        "            if isinstance(module,torch.nn.Linear):\n",
        "                prune.l1_unstructured(module, name='weight', amount=amount)\n",
        "\n",
        "    def quantize_model(self):\n",
        "        self.model.qconfig = torch.quantization.default_qconfig\n",
        "        self.model = torch.quantization.prepare(self.model)\n",
        "        self.model = torch.quantization.convert(self.model)\n",
        "\n",
        "    def evaluate(self, train_args):\n",
        "        trainer = Trainer(\n",
        "            model=self.model,\n",
        "            args=train_args,\n",
        "            train_dataset=self.tokenized_dataset['train'],\n",
        "            eval_dataset=self.tokenized_dataset['test'],\n",
        "            compute_metrics=self.metric_fn)\n",
        "\n",
        "        predictions = trainer.predict(self.tokenized_dataset['test'])\n",
        "        result_dict = self.metric_fn(predictions)\n",
        "        for k,v in result_dict.items():\n",
        "            print(f'{k} value: {v}')\n",
        "\n",
        "    def kl(self,hidden_dim,num_layers,num_heads,epochs, alpha_teacher, lr, loss_func):\n",
        "        embedding_matrix = self.model.bert.embeddings.word_embeddings.weight\n",
        "        embedding_dim = embedding_matrix.size(1)\n",
        "        vocab_size = embedding_matrix.size(0)\n",
        "        Student = Student_Calssifier(vocab_size, embedding_dim, hidden_dim, num_layers, num_heads, self.num_classes)\n",
        "        Student.embedding.weight.data.copy_(embedding_matrix)\n",
        "        Teacher = copy.deepcopy(self.model).to(device)\n",
        "        Student.to(device)\n",
        "        data = self.tokenized_dataset['train']\n",
        "\n",
        "        Student = kl_training(Student, Teacher, epochs, alpha_teacher, lr, loss_func, data)\n",
        "        return Student\n",
        "\n",
        "    def save_model(self, model_name):\n",
        "        torch.save(self.model.state_dict(), f'{model_name}.pt')\n",
        "\n"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1862876c",
      "metadata": {
        "id": "1862876c"
      },
      "source": [
        "# <u>Model 1 - BERT Base Uncased<u>\n",
        "## Model 1  Definition and Tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 13,
      "id": "816bf01b",
      "metadata": {
        "id": "816bf01b",
        "outputId": "d64f97f8-b916-4661-84bd-a0715e1464cf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Collecting requests==2.27.1\n",
            "  Using cached requests-2.27.1-py2.py3-none-any.whl (63 kB)\n",
            "Collecting urllib3<1.27,>=1.21.1 (from requests==2.27.1)\n",
            "  Downloading urllib3-1.26.16-py2.py3-none-any.whl (143 kB)\n",
            "                                              0.0/143.1 kB ? eta -:--:--\n",
            "     --------                                30.7/143.1 kB 1.4 MB/s eta 0:00:01\n",
            "     ------------------                    71.7/143.1 kB 653.6 kB/s eta 0:00:01\n",
            "     ----------------------------         112.6/143.1 kB 819.2 kB/s eta 0:00:01\n",
            "     ------------------------------------ 143.1/143.1 kB 851.7 kB/s eta 0:00:00\n",
            "Requirement already satisfied: certifi>=2017.4.17 in c:\\python310\\lib\\site-packages (from requests==2.27.1) (2023.5.7)\n",
            "Collecting charset-normalizer~=2.0.0 (from requests==2.27.1)\n",
            "  Using cached charset_normalizer-2.0.12-py3-none-any.whl (39 kB)\n",
            "Requirement already satisfied: idna<4,>=2.5 in c:\\python310\\lib\\site-packages (from requests==2.27.1) (3.4)\n",
            "Installing collected packages: urllib3, charset-normalizer, requests\n",
            "  Attempting uninstall: urllib3\n",
            "    Found existing installation: urllib3 2.0.3\n",
            "    Uninstalling urllib3-2.0.3:\n",
            "      Successfully uninstalled urllib3-2.0.3\n",
            "  Attempting uninstall: charset-normalizer\n",
            "    Found existing installation: charset-normalizer 3.1.0\n",
            "    Uninstalling charset-normalizer-3.1.0:\n",
            "      Successfully uninstalled charset-normalizer-3.1.0\n",
            "Note: you may need to restart the kernel to use updated packages.\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR: Could not install packages due to an OSError: [WinError 5] Access is denied: 'C:\\\\Python310\\\\Lib\\\\site-packages\\\\~harset_normalizer\\\\md.cp310-win_amd64.pyd'\n",
            "Consider using the `--user` option or check the permissions.\n",
            "\n"
          ]
        }
      ],
      "source": [
        "pip install requests==2.27.1"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 14,
      "id": "90a33a38",
      "metadata": {
        "id": "90a33a38"
      },
      "outputs": [],
      "source": [
        "os.environ['CURL_CA_BUNDLE'] = ''"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 15,
      "id": "1ee9ca97",
      "metadata": {
        "id": "1ee9ca97",
        "outputId": "62c9f96d-11a5-4dd3-babb-525eefbe0ee0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        }
      ],
      "source": [
        "model_name_1 = \"bert-base-uncased\"\n",
        "num_of_classes = 62\n",
        "bert_model = Model(model_name_1, datasets, num_of_classes)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 16,
      "id": "24def1d0",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            ""
          ]
        },
        "id": "24def1d0",
        "outputId": "2f15ad4e-c356-41be-8c52-427f2e1a26a0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "                                                                     \r"
          ]
        }
      ],
      "source": [
        "token_args = {\"max_length\": 64, \"truncation\": True, \"padding\": \"max_length\"}\n",
        "bert_model.tokenize(token_args)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "c70b3308",
      "metadata": {
        "id": "c70b3308"
      },
      "source": [
        "## Model 1 Hyperparameter Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "c7823d44",
      "metadata": {
        "id": "c7823d44",
        "outputId": "b23ae976-7631-4bac-c03f-1eb86c5e82b6"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.wandb.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'api.wandb.ai'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliyag\u001b[0m (\u001b[33mdelta_lxr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230612_235625-45owyuws</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/DeepLearning/runs/45owyuws' target=\"_blank\">divine-sun-17</a></strong> to <a href='https://wandb.ai/delta_lxr/DeepLearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/DeepLearning' target=\"_blank\">https://wandb.ai/delta_lxr/DeepLearning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/DeepLearning/runs/45owyuws' target=\"_blank\">https://wandb.ai/delta_lxr/DeepLearning/runs/45owyuws</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.init(project=\"DeepLearning\")\n",
        "\n",
        "train_args = TrainingArguments(output_dir=output_dir,\n",
        "                             overwrite_output_dir=True,\n",
        "                             greater_is_better=True,\n",
        "                             evaluation_strategy='epoch',\n",
        "                             do_train=True,\n",
        "                             logging_strategy='epoch',\n",
        "                             save_strategy='no',\n",
        "                             report_to='wandb')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9522941b",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "3b586a453c3043fab3952b63ebda6a2b",
            "40de89c8618843bf9d92f7193507bcdf",
            "779191a5c7174fa7810650adb8e165fd",
            "8837dd7c7568450584f245c0cfccad58",
            "e787d319e4a94f4ba728c089315b532d",
            "ef6c143ecde148beb20dbecab2ee4b8c",
            "2818757f77b3400d95756dbccab2a968",
            "166b1b629eba4f29afd7a3daf5b05731",
            "8e299fd3e4ea4a62beb9fbb6ca031bb9",
            "0cd8abb61a0a4d32b0a30f778f6c5be0",
            "249507f631d2447a80aea83dd00f6f76",
            "9ae2731078444a3dad454b93ca7ff845",
            "50056dcf14c7498082b68313770f1502"
          ]
        },
        "id": "9522941b",
        "outputId": "46ad4e6c-0d7f-49d8-8d64-cab152709ec0"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:359: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n",
            "  warnings.warn(\n",
            "\u001b[32m[I 2023-06-12 23:56:45,096]\u001b[0m A new study created in memory with name: no-name-5b015cbb-7d7d-4411-8219-748edfad20d2\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "3b586a453c3043fab3952b63ebda6a2b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">divine-sun-17</strong> at: <a href='https://wandb.ai/delta_lxr/DeepLearning/runs/45owyuws' target=\"_blank\">https://wandb.ai/delta_lxr/DeepLearning/runs/45owyuws</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230612_235625-45owyuws\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "40de89c8618843bf9d92f7193507bcdf",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01718333333362049, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230612_235650-40vz4787</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/40vz4787' target=\"_blank\">valiant-firefly-94</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/40vz4787' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/40vz4787</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='65100' max='65100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [65100/65100 1:22:21, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.012800</td>\n",
              "      <td>2.324755</td>\n",
              "      <td>0.385335</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.162800</td>\n",
              "      <td>1.966571</td>\n",
              "      <td>0.473018</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.874100</td>\n",
              "      <td>1.819094</td>\n",
              "      <td>0.513033</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.702700</td>\n",
              "      <td>1.740530</td>\n",
              "      <td>0.531638</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.582300</td>\n",
              "      <td>1.689133</td>\n",
              "      <td>0.551586</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.505800</td>\n",
              "      <td>1.658887</td>\n",
              "      <td>0.560356</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.461000</td>\n",
              "      <td>1.650897</td>\n",
              "      <td>0.562540</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-13 01:19:22,168]\u001b[0m Trial 0 finished with value: 0.5625395808513218 and parameters: {'learning_rate': 4.183323918010571e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 1, 'warmup_steps': 115, 'weight_decay': 0.08914088722040585, 'per_device_eval_batch_size': 16}. Best is trial 0 with value: 0.5625395808513218.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▆▇███</td></tr><tr><td>eval/loss</td><td>█▄▃▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▁▇█████</td></tr><tr><td>eval/samples_per_second</td><td>█▂▁▁▁▁▁</td></tr><tr><td>eval/steps_per_second</td><td>█▂▁▁▁▁▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.56254</td></tr><tr><td>eval/loss</td><td>1.6509</td></tr><tr><td>eval/runtime</td><td>37.1272</td></tr><tr><td>eval/samples_per_second</td><td>1001.96</td></tr><tr><td>eval/steps_per_second</td><td>62.623</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>65100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.461</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>1.9002</td></tr><tr><td>train/train_runtime</td><td>4957.0582</td></tr><tr><td>train/train_samples_per_second</td><td>210.125</td></tr><tr><td>train/train_steps_per_second</td><td>13.133</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">valiant-firefly-94</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/40vz4787' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/40vz4787</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230612_235650-40vz4787\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230613_011927-9sc3rrc8</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/9sc3rrc8' target=\"_blank\">sandy-sky-95</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/9sc3rrc8' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/9sc3rrc8</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32550' max='32550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32550/32550 1:08:45, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.527100</td>\n",
              "      <td>1.546883</td>\n",
              "      <td>0.591352</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.112200</td>\n",
              "      <td>1.424080</td>\n",
              "      <td>0.635877</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.815000</td>\n",
              "      <td>1.397030</td>\n",
              "      <td>0.658872</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.600300</td>\n",
              "      <td>1.433538</td>\n",
              "      <td>0.669496</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.434200</td>\n",
              "      <td>1.475755</td>\n",
              "      <td>0.679990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.317900</td>\n",
              "      <td>1.537445</td>\n",
              "      <td>0.685465</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.243000</td>\n",
              "      <td>1.563465</td>\n",
              "      <td>0.686887</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-13 02:28:20,853]\u001b[0m Trial 1 finished with value: 0.6868869998350791 and parameters: {'learning_rate': 2.6741215081649235e-05, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 2, 'warmup_steps': 109, 'weight_decay': 0.09151740119576889, 'per_device_eval_batch_size': 16}. Best is trial 1 with value: 0.6868869998350791.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "779191a5c7174fa7810650adb8e165fd",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▆▇▇██</td></tr><tr><td>eval/loss</td><td>▇▂▁▃▄▇█</td></tr><tr><td>eval/runtime</td><td>▅█▁▆▁█▆</td></tr><tr><td>eval/samples_per_second</td><td>▄▁█▃█▁▃</td></tr><tr><td>eval/steps_per_second</td><td>▄▁█▃█▁▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▃▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68689</td></tr><tr><td>eval/loss</td><td>1.56347</td></tr><tr><td>eval/runtime</td><td>37.4174</td></tr><tr><td>eval/samples_per_second</td><td>994.191</td></tr><tr><td>eval/steps_per_second</td><td>62.137</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>32550</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.243</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.72138</td></tr><tr><td>train/train_runtime</td><td>4138.6667</td></tr><tr><td>train/train_samples_per_second</td><td>251.675</td></tr><tr><td>train/train_steps_per_second</td><td>7.865</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">sandy-sky-95</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/9sc3rrc8' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/9sc3rrc8</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230613_011927-9sc3rrc8\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230613_022826-kf9wkl2a</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/kf9wkl2a' target=\"_blank\">rich-serenity-96</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/kf9wkl2a' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/kf9wkl2a</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16275' max='16275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16275/16275 1:00:49, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.204500</td>\n",
              "      <td>1.660191</td>\n",
              "      <td>0.683830</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.135300</td>\n",
              "      <td>1.756216</td>\n",
              "      <td>0.683789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.084900</td>\n",
              "      <td>1.877569</td>\n",
              "      <td>0.684486</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.055300</td>\n",
              "      <td>1.976170</td>\n",
              "      <td>0.684927</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.036800</td>\n",
              "      <td>2.073597</td>\n",
              "      <td>0.689748</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.028800</td>\n",
              "      <td>2.131151</td>\n",
              "      <td>0.689641</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.039000</td>\n",
              "      <td>2.142794</td>\n",
              "      <td>0.688738</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-13 03:29:23,919]\u001b[0m Trial 2 finished with value: 0.6887378135934601 and parameters: {'learning_rate': 1.749735397672745e-05, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 4, 'warmup_steps': 97, 'weight_decay': 0.016675364906410122, 'per_device_eval_batch_size': 16}. Best is trial 2 with value: 0.6887378135934601.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8837dd7c7568450584f245c0cfccad58",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▁▂▂██▇</td></tr><tr><td>eval/loss</td><td>▁▂▄▆▇██</td></tr><tr><td>eval/runtime</td><td>▂▃█▃▃▁▁</td></tr><tr><td>eval/samples_per_second</td><td>▇▆▁▆▆██</td></tr><tr><td>eval/steps_per_second</td><td>▇▆▁▆▆██</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68874</td></tr><tr><td>eval/loss</td><td>2.14279</td></tr><tr><td>eval/runtime</td><td>37.2199</td></tr><tr><td>eval/samples_per_second</td><td>999.467</td></tr><tr><td>eval/steps_per_second</td><td>62.467</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>16275</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.039</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.08352</td></tr><tr><td>train/train_runtime</td><td>3663.049</td></tr><tr><td>train/train_samples_per_second</td><td>284.353</td></tr><tr><td>train/train_steps_per_second</td><td>4.443</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">rich-serenity-96</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/kf9wkl2a' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/kf9wkl2a</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230613_022826-kf9wkl2a\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230613_032930-s4jmjigt</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/s4jmjigt' target=\"_blank\">honest-fire-97</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/s4jmjigt' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/s4jmjigt</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21700' max='21700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21700/21700 1:03:26, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.008300</td>\n",
              "      <td>2.179108</td>\n",
              "      <td>0.692447</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.006700</td>\n",
              "      <td>2.209583</td>\n",
              "      <td>0.690939</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.004500</td>\n",
              "      <td>2.240933</td>\n",
              "      <td>0.691621</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.003400</td>\n",
              "      <td>2.274277</td>\n",
              "      <td>0.691419</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>2.306708</td>\n",
              "      <td>0.690071</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.002800</td>\n",
              "      <td>2.327006</td>\n",
              "      <td>0.690720</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.013000</td>\n",
              "      <td>2.331071</td>\n",
              "      <td>0.690011</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-13 04:33:05,149]\u001b[0m Trial 3 finished with value: 0.6900108258517192 and parameters: {'learning_rate': 1.947733969457909e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 3, 'warmup_steps': 146, 'weight_decay': 0.08874974883831581, 'per_device_eval_batch_size': 16}. Best is trial 3 with value: 0.6900108258517192.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e787d319e4a94f4ba728c089315b532d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>█▄▆▅▁▃▁</td></tr><tr><td>eval/loss</td><td>▁▂▄▅▇██</td></tr><tr><td>eval/runtime</td><td>█▆▁▅▆█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▃█▄▃▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁▃█▄▃▁█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>▅▄▂▁▁▁█</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.69001</td></tr><tr><td>eval/loss</td><td>2.33107</td></tr><tr><td>eval/runtime</td><td>37.2921</td></tr><tr><td>eval/samples_per_second</td><td>997.53</td></tr><tr><td>eval/steps_per_second</td><td>62.346</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>21700</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.013</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.00592</td></tr><tr><td>train/train_runtime</td><td>3821.2161</td></tr><tr><td>train/train_samples_per_second</td><td>272.583</td></tr><tr><td>train/train_steps_per_second</td><td>5.679</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">honest-fire-97</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/s4jmjigt' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/s4jmjigt</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230613_032930-s4jmjigt\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230613_043311-j8is1tto</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/j8is1tto' target=\"_blank\">major-galaxy-98</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/j8is1tto' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/j8is1tto</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16275' max='16275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16275/16275 1:00:48, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.011200</td>\n",
              "      <td>2.548951</td>\n",
              "      <td>0.684470</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.012800</td>\n",
              "      <td>2.616982</td>\n",
              "      <td>0.683974</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.009800</td>\n",
              "      <td>2.711909</td>\n",
              "      <td>0.685860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.007000</td>\n",
              "      <td>2.735688</td>\n",
              "      <td>0.691193</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.005100</td>\n",
              "      <td>2.815058</td>\n",
              "      <td>0.689085</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.005500</td>\n",
              "      <td>2.829425</td>\n",
              "      <td>0.689180</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.018000</td>\n",
              "      <td>2.835057</td>\n",
              "      <td>0.689578</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-13 05:34:08,352]\u001b[0m Trial 4 finished with value: 0.6895778609936061 and parameters: {'learning_rate': 1.5942436644489087e-05, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 4, 'warmup_steps': 131, 'weight_decay': 0.0747548214676366, 'per_device_eval_batch_size': 16}. Best is trial 3 with value: 0.6900108258517192.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ef6c143ecde148beb20dbecab2ee4b8c",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▁▃█▆▆▆</td></tr><tr><td>eval/loss</td><td>▁▃▅▆███</td></tr><tr><td>eval/runtime</td><td>█▆▅▆▁▂▇</td></tr><tr><td>eval/samples_per_second</td><td>▁▃▄▃█▇▂</td></tr><tr><td>eval/steps_per_second</td><td>▁▃▄▃█▇▂</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>▄▅▄▂▁▁█</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68958</td></tr><tr><td>eval/loss</td><td>2.83506</td></tr><tr><td>eval/runtime</td><td>37.7768</td></tr><tr><td>eval/samples_per_second</td><td>984.732</td></tr><tr><td>eval/steps_per_second</td><td>61.546</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>16275</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.018</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.00991</td></tr><tr><td>train/train_runtime</td><td>3663.1884</td></tr><tr><td>train/train_samples_per_second</td><td>284.342</td></tr><tr><td>train/train_steps_per_second</td><td>4.443</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">major-galaxy-98</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/j8is1tto' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/j8is1tto</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230613_043311-j8is1tto\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230613_053413-38t0x5o6</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/38t0x5o6' target=\"_blank\">blooming-water-99</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/38t0x5o6' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/38t0x5o6</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21700' max='21700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21700/21700 1:03:25, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.001100</td>\n",
              "      <td>2.836236</td>\n",
              "      <td>0.694702</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>2.860836</td>\n",
              "      <td>0.690769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>2.871124</td>\n",
              "      <td>0.692341</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>2.887267</td>\n",
              "      <td>0.692364</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>2.913437</td>\n",
              "      <td>0.690063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>2.922227</td>\n",
              "      <td>0.691025</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.006500</td>\n",
              "      <td>2.926505</td>\n",
              "      <td>0.690831</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-13 06:37:47,665]\u001b[0m Trial 5 finished with value: 0.6908305705677684 and parameters: {'learning_rate': 1.8325249831728925e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 3, 'warmup_steps': 181, 'weight_decay': 0.014021528948180751, 'per_device_eval_batch_size': 16}. Best is trial 5 with value: 0.6908305705677684.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2818757f77b3400d95756dbccab2a968",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>█▂▄▄▁▂▂</td></tr><tr><td>eval/loss</td><td>▁▃▄▅▇██</td></tr><tr><td>eval/runtime</td><td>█▁▃█▇▁▅</td></tr><tr><td>eval/samples_per_second</td><td>▁█▆▁▂█▄</td></tr><tr><td>eval/steps_per_second</td><td>▁█▆▁▂█▄</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>▂▁▁▁▁▁█</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.69083</td></tr><tr><td>eval/loss</td><td>2.92651</td></tr><tr><td>eval/runtime</td><td>37.6288</td></tr><tr><td>eval/samples_per_second</td><td>988.606</td></tr><tr><td>eval/steps_per_second</td><td>61.788</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>21700</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0065</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.00137</td></tr><tr><td>train/train_runtime</td><td>3819.2999</td></tr><tr><td>train/train_samples_per_second</td><td>272.72</td></tr><tr><td>train/train_steps_per_second</td><td>5.682</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">blooming-water-99</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/38t0x5o6' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/38t0x5o6</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230613_053413-38t0x5o6\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "166b1b629eba4f29afd7a3daf5b05731",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.017200000000108653, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230613_063753-jtoxs5ic</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/jtoxs5ic' target=\"_blank\">dauntless-aardvark-100</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/jtoxs5ic' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/jtoxs5ic</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='9300' max='65100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 9300/65100 11:55 < 1:11:31, 13.00 it/s, Epoch 1/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.080100</td>\n",
              "      <td>3.163306</td>\n",
              "      <td>0.674801</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-13 06:49:57,561]\u001b[0m Trial 6 pruned. \u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "8e299fd3e4ea4a62beb9fbb6ca031bb9",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.6748</td></tr><tr><td>eval/loss</td><td>3.16331</td></tr><tr><td>eval/runtime</td><td>38.0542</td></tr><tr><td>eval/samples_per_second</td><td>977.553</td></tr><tr><td>eval/steps_per_second</td><td>61.097</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>9300</td></tr><tr><td>train/learning_rate</td><td>2e-05</td></tr><tr><td>train/loss</td><td>0.0801</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dauntless-aardvark-100</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/jtoxs5ic' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/jtoxs5ic</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230613_063753-jtoxs5ic\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "0cd8abb61a0a4d32b0a30f778f6c5be0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01691666666592937, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230613_065003-sd1a4ngl</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/sd1a4ngl' target=\"_blank\">vibrant-firefly-101</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/sd1a4ngl' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/sd1a4ngl</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3100' max='21700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3100/21700 09:04 < 54:27, 5.69 it/s, Epoch 1/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.048000</td>\n",
              "      <td>3.053157</td>\n",
              "      <td>0.683086</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-13 06:59:16,410]\u001b[0m Trial 7 pruned. \u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "249507f631d2447a80aea83dd00f6f76",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68309</td></tr><tr><td>eval/loss</td><td>3.05316</td></tr><tr><td>eval/runtime</td><td>37.8928</td></tr><tr><td>eval/samples_per_second</td><td>981.716</td></tr><tr><td>eval/steps_per_second</td><td>61.357</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>3100</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.048</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">vibrant-firefly-101</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/sd1a4ngl' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/sd1a4ngl</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230613_065003-sd1a4ngl\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230613_065921-0qk0ypoi</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/0qk0ypoi' target=\"_blank\">dry-water-102</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/0qk0ypoi' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/0qk0ypoi</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='3100' max='21700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 3100/21700 09:04 < 54:27, 5.69 it/s, Epoch 1/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.007600</td>\n",
              "      <td>3.086411</td>\n",
              "      <td>0.686275</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-13 07:08:33,818]\u001b[0m Trial 8 pruned. \u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "9ae2731078444a3dad454b93ca7ff845",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68628</td></tr><tr><td>eval/loss</td><td>3.08641</td></tr><tr><td>eval/runtime</td><td>37.5366</td></tr><tr><td>eval/samples_per_second</td><td>991.033</td></tr><tr><td>eval/steps_per_second</td><td>61.94</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>3100</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0076</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dry-water-102</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/0qk0ypoi' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/0qk0ypoi</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230613_065921-0qk0ypoi\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230613_070839-0xr6tbqr</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/0xr6tbqr' target=\"_blank\">jumping-disco-103</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/0xr6tbqr' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/0xr6tbqr</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='2325' max='16275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 2325/16275 08:41 < 52:09, 4.46 it/s, Epoch 1/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>3.164068</td>\n",
              "      <td>0.683753</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-13 07:17:28,779]\u001b[0m Trial 9 pruned. \u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "50056dcf14c7498082b68313770f1502",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁</td></tr><tr><td>eval/loss</td><td>▁</td></tr><tr><td>eval/runtime</td><td>▁</td></tr><tr><td>eval/samples_per_second</td><td>▁</td></tr><tr><td>eval/steps_per_second</td><td>▁</td></tr><tr><td>train/epoch</td><td>▁▁</td></tr><tr><td>train/global_step</td><td>▁▁</td></tr><tr><td>train/learning_rate</td><td>▁</td></tr><tr><td>train/loss</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68375</td></tr><tr><td>eval/loss</td><td>3.16407</td></tr><tr><td>eval/runtime</td><td>37.5049</td></tr><tr><td>eval/samples_per_second</td><td>991.869</td></tr><tr><td>eval/steps_per_second</td><td>61.992</td></tr><tr><td>train/epoch</td><td>1.0</td></tr><tr><td>train/global_step</td><td>2325</td></tr><tr><td>train/learning_rate</td><td>1e-05</td></tr><tr><td>train/loss</td><td>0.0017</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">jumping-disco-103</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/0xr6tbqr' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/0xr6tbqr</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230613_070839-0xr6tbqr\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "bert-base-uncased chosen hyperparameters:\n",
            "{'learning_rate': 1.8325249831728925e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 3, 'warmup_steps': 181, 'weight_decay': 0.014021528948180751, 'per_device_eval_batch_size': 16}\n"
          ]
        }
      ],
      "source": [
        "bert_model.hyper_parameters_search(train_args)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "41e6b4ba",
      "metadata": {
        "id": "41e6b4ba"
      },
      "source": [
        "## Model 1 Train on best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e9ba550",
      "metadata": {
        "id": "9e9ba550",
        "outputId": "0da87567-bd7b-4272-ca55-03d7d5048550",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='48' max='46500' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [   48/46500 00:08 < 2:17:09, 5.64 it/s, Epoch 0.02/15]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[28], line 32\u001b[0m\n\u001b[0;32m      1\u001b[0m best_args \u001b[38;5;241m=\u001b[39m TrainingArguments(output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m      2\u001b[0m                          overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      3\u001b[0m                          per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m                          warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m181\u001b[39m,\n\u001b[0;32m     15\u001b[0m                          report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m eval_args \u001b[38;5;241m=\u001b[39m TrainingArguments(output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m     18\u001b[0m                          overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m                          per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m                          warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m181\u001b[39m,\n\u001b[0;32m     30\u001b[0m                          report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mbert_model\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_args\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[11], line 28\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, train_args)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_args):\n\u001b[0;32m     21\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     22\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m     23\u001b[0m         args\u001b[38;5;241m=\u001b[39mtrain_args,\n\u001b[0;32m     24\u001b[0m         train_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     25\u001b[0m         eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     26\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_fn)\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1929\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1927\u001b[0m         tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtraining_step(model, inputs)\n\u001b[0;32m   1928\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 1929\u001b[0m     tr_loss_step \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtraining_step\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1931\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (\n\u001b[0;32m   1932\u001b[0m     args\u001b[38;5;241m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1933\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1934\u001b[0m     \u001b[38;5;129;01mand\u001b[39;00m (torch\u001b[38;5;241m.\u001b[39misnan(tr_loss_step) \u001b[38;5;129;01mor\u001b[39;00m torch\u001b[38;5;241m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1935\u001b[0m ):\n\u001b[0;32m   1936\u001b[0m     \u001b[38;5;66;03m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1937\u001b[0m     tr_loss \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m tr_loss \u001b[38;5;241m/\u001b[39m (\u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step \u001b[38;5;241m-\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2717\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2715\u001b[0m     loss \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed\u001b[38;5;241m.\u001b[39mbackward(loss)\n\u001b[0;32m   2716\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m-> 2717\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2719\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m loss\u001b[38;5;241m.\u001b[39mdetach()\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "best_args = TrainingArguments(output_dir=output_dir,\n",
        "                         overwrite_output_dir=True,\n",
        "                         per_device_train_batch_size=16,\n",
        "                         per_device_eval_batch_size=16,\n",
        "                         seed = 9,\n",
        "                         learning_rate=1.8325249831728925e-06,\n",
        "                         weight_decay=0.014021528948180751,\n",
        "                         greater_is_better=True,\n",
        "                         evaluation_strategy='epoch',\n",
        "                         do_train=True,\n",
        "                         num_train_epochs=15,\n",
        "                         gradient_accumulation_steps=3,\n",
        "                         logging_strategy='epoch',\n",
        "                         warmup_steps=181,\n",
        "                         report_to='wandb')\n",
        "\n",
        "eval_args = TrainingArguments(output_dir=output_dir,\n",
        "                         overwrite_output_dir=True,\n",
        "                         per_device_train_batch_size=16,\n",
        "                         per_device_eval_batch_size=16,\n",
        "                          seed = 9,\n",
        "                         learning_rate=1.8325249831728925e-06,\n",
        "                         weight_decay=0.014021528948180751,\n",
        "                         greater_is_better=True,\n",
        "                         evaluation_strategy='epoch',\n",
        "                         do_train=False,\n",
        "                         gradient_accumulation_steps=3,\n",
        "                         logging_strategy='epoch',\n",
        "                         warmup_steps=181,\n",
        "                         report_to='wandb')\n",
        "\n",
        "bert_model.train(best_args)\n",
        "# save model's state dict\n",
        "bert_model.save_model('bert_regular')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a90313a8",
      "metadata": {
        "id": "a90313a8"
      },
      "outputs": [],
      "source": [
        "#bert_model.save_model('bert_regular')\n",
        "torch.save(bert_model.model.state_dict(),\"bert_regular.pt\")"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "e3e7d6a2",
      "metadata": {
        "id": "e3e7d6a2"
      },
      "source": [
        "## Best model summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "04fe6286",
      "metadata": {
        "id": "04fe6286",
        "outputId": "cd7aa790-1af0-4cb3-9ec9-58fd1cbf6ca5",
        "scrolled": true
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1 value: 0.6869010703006206\n",
            "438.19 MB\n",
            "+----------------------------------------------+----------------+\n",
            "|                  Layer Name                  | Sum of Weights |\n",
            "+----------------------------------------------+----------------+\n",
            "|  bert.encoder.layer.0.attention.self.query   |    18635.98    |\n",
            "|   bert.encoder.layer.0.attention.self.key    |   18326.658    |\n",
            "|  bert.encoder.layer.0.attention.self.value   |   12523.816    |\n",
            "| bert.encoder.layer.0.attention.output.dense  |   12118.736    |\n",
            "|   bert.encoder.layer.0.intermediate.dense    |    65533.0     |\n",
            "|      bert.encoder.layer.0.output.dense       |   61711.406    |\n",
            "|  bert.encoder.layer.1.attention.self.query   |   18492.051    |\n",
            "|   bert.encoder.layer.1.attention.self.key    |   18417.031    |\n",
            "|  bert.encoder.layer.1.attention.self.value   |   12342.176    |\n",
            "| bert.encoder.layer.1.attention.output.dense  |   11883.429    |\n",
            "|   bert.encoder.layer.1.intermediate.dense    |   68518.859    |\n",
            "|      bert.encoder.layer.1.output.dense       |   64747.352    |\n",
            "|  bert.encoder.layer.2.attention.self.query   |   20233.092    |\n",
            "|   bert.encoder.layer.2.attention.self.key    |   19771.156    |\n",
            "|  bert.encoder.layer.2.attention.self.value   |   12134.582    |\n",
            "| bert.encoder.layer.2.attention.output.dense  |   11646.666    |\n",
            "|   bert.encoder.layer.2.intermediate.dense    |   69496.422    |\n",
            "|      bert.encoder.layer.2.output.dense       |   65542.156    |\n",
            "|  bert.encoder.layer.3.attention.self.query   |   18721.574    |\n",
            "|   bert.encoder.layer.3.attention.self.key    |   18603.934    |\n",
            "|  bert.encoder.layer.3.attention.self.value   |   13498.661    |\n",
            "| bert.encoder.layer.3.attention.output.dense  |   12504.776    |\n",
            "|   bert.encoder.layer.3.intermediate.dense    |   70494.719    |\n",
            "|      bert.encoder.layer.3.output.dense       |   66640.859    |\n",
            "|  bert.encoder.layer.4.attention.self.query   |   18392.539    |\n",
            "|   bert.encoder.layer.4.attention.self.key    |   18281.725    |\n",
            "|  bert.encoder.layer.4.attention.self.value   |   15164.183    |\n",
            "| bert.encoder.layer.4.attention.output.dense  |   14003.158    |\n",
            "|   bert.encoder.layer.4.intermediate.dense    |   70943.602    |\n",
            "|      bert.encoder.layer.4.output.dense       |   67511.156    |\n",
            "|  bert.encoder.layer.5.attention.self.query   |   18950.422    |\n",
            "|   bert.encoder.layer.5.attention.self.key    |   18910.316    |\n",
            "|  bert.encoder.layer.5.attention.self.value   |   15116.175    |\n",
            "| bert.encoder.layer.5.attention.output.dense  |   14284.295    |\n",
            "|   bert.encoder.layer.5.intermediate.dense    |   70699.797    |\n",
            "|      bert.encoder.layer.5.output.dense       |   66688.781    |\n",
            "|  bert.encoder.layer.6.attention.self.query   |   18834.814    |\n",
            "|   bert.encoder.layer.6.attention.self.key    |    18796.41    |\n",
            "|  bert.encoder.layer.6.attention.self.value   |    14765.84    |\n",
            "| bert.encoder.layer.6.attention.output.dense  |   13907.414    |\n",
            "|   bert.encoder.layer.6.intermediate.dense    |    71173.0     |\n",
            "|      bert.encoder.layer.6.output.dense       |   66221.602    |\n",
            "|  bert.encoder.layer.7.attention.self.query   |   19003.398    |\n",
            "|   bert.encoder.layer.7.attention.self.key    |   19039.564    |\n",
            "|  bert.encoder.layer.7.attention.self.value   |   14356.958    |\n",
            "| bert.encoder.layer.7.attention.output.dense  |   13756.449    |\n",
            "|   bert.encoder.layer.7.intermediate.dense    |   68646.031    |\n",
            "|      bert.encoder.layer.7.output.dense       |   64518.359    |\n",
            "|  bert.encoder.layer.8.attention.self.query   |   19297.672    |\n",
            "|   bert.encoder.layer.8.attention.self.key    |   19324.361    |\n",
            "|  bert.encoder.layer.8.attention.self.value   |   15466.838    |\n",
            "| bert.encoder.layer.8.attention.output.dense  |   14595.388    |\n",
            "|   bert.encoder.layer.8.intermediate.dense    |   68704.969    |\n",
            "|      bert.encoder.layer.8.output.dense       |   64519.699    |\n",
            "|  bert.encoder.layer.9.attention.self.query   |   20129.516    |\n",
            "|   bert.encoder.layer.9.attention.self.key    |   20071.455    |\n",
            "|  bert.encoder.layer.9.attention.self.value   |   15109.768    |\n",
            "| bert.encoder.layer.9.attention.output.dense  |   14322.646    |\n",
            "|   bert.encoder.layer.9.intermediate.dense    |   69303.719    |\n",
            "|      bert.encoder.layer.9.output.dense       |   66801.719    |\n",
            "|  bert.encoder.layer.10.attention.self.query  |   20111.475    |\n",
            "|   bert.encoder.layer.10.attention.self.key   |    20020.32    |\n",
            "|  bert.encoder.layer.10.attention.self.value  |   15407.669    |\n",
            "| bert.encoder.layer.10.attention.output.dense |   14561.625    |\n",
            "|   bert.encoder.layer.10.intermediate.dense   |   68532.172    |\n",
            "|      bert.encoder.layer.10.output.dense      |   65856.359    |\n",
            "|  bert.encoder.layer.11.attention.self.query  |   19833.352    |\n",
            "|   bert.encoder.layer.11.attention.self.key   |   19515.104    |\n",
            "|  bert.encoder.layer.11.attention.self.value  |    17132.32    |\n",
            "| bert.encoder.layer.11.attention.output.dense |   16174.036    |\n",
            "|   bert.encoder.layer.11.intermediate.dense   |   69100.672    |\n",
            "|      bert.encoder.layer.11.output.dense      |   63065.332    |\n",
            "|              bert.pooler.dense               |   13287.057    |\n",
            "|                  classifier                  |    1513.709    |\n",
            "|             Total Sum of Weights             |  2426264.032   |\n",
            "+----------------------------------------------+----------------+\n"
          ]
        }
      ],
      "source": [
        "bert_model.model.load_state_dict(torch.load(Path(home_dir, 'bert_regular.pt')))\n",
        "bert_model.evaluate(eval_args)\n",
        "bert_model.print_model_size()\n",
        "bert_model.print_model()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "879d9c7e",
      "metadata": {
        "id": "879d9c7e"
      },
      "source": [
        "## a. Model 1 Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "e4198c1e",
      "metadata": {
        "id": "e4198c1e",
        "outputId": "c874d6cc-698f-435e-c07b-5cb1d88c485d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "Loading cached processed dataset at C:\\Users\\liyag\\.cache\\huggingface\\datasets\\csv\\default-f0e020e5f3a5fb7b\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d0c596d113269e70.arrow\n",
            "Loading cached processed dataset at C:\\Users\\liyag\\.cache\\huggingface\\datasets\\csv\\default-f0e020e5f3a5fb7b\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-58ee211d4801a52d.arrow\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 30,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "pruned_bert = Model(model_name_1, datasets, num_of_classes)\n",
        "pruned_bert.tokenize(token_args)\n",
        "\n",
        "# Load the model's best parameters\n",
        "pruned_bert.model.load_state_dict(torch.load(Path(home_dir, 'bert_regular.pt')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7e0fe4cb",
      "metadata": {
        "id": "7e0fe4cb",
        "outputId": "53d996b4-fcb7-4959-8631-f252f98d586d",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='18600' max='18600' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [18600/18600 25:09, Epoch 1/1]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.655800</td>\n",
              "      <td>2.059624</td>\n",
              "      <td>0.677904</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Prune and train one epoch\n",
        "pruned_bert.pruning(0.2)\n",
        "train_one_epoch_args = TrainingArguments(output_dir=output_dir,\n",
        "                             overwrite_output_dir=True,\n",
        "                             greater_is_better=True,\n",
        "                             evaluation_strategy='epoch',\n",
        "                             do_train=True,\n",
        "                             logging_strategy='epoch',\n",
        "                             num_train_epochs=1,\n",
        "                             save_strategy='no')\n",
        "\n",
        "pruned_bert.train(train_one_epoch_args)\n",
        "# save model's state dict\n",
        "pruned_bert.save_model('bert_pruning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "9e6c79ac",
      "metadata": {
        "id": "9e6c79ac",
        "outputId": "1dfe60b6-7b9d-46cd-c653-02e5722046b1"
      },
      "outputs": [
        {
          "data": {
            "text/html": [],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "f1 value: 0.677904232049942\n",
            "780.50 MB\n",
            "+----------------------------------------------+----------------+\n",
            "|                  Layer Name                  | Sum of Weights |\n",
            "+----------------------------------------------+----------------+\n",
            "|  bert.encoder.layer.0.attention.self.query   |   18088.371    |\n",
            "|   bert.encoder.layer.0.attention.self.key    |    17797.49    |\n",
            "|  bert.encoder.layer.0.attention.self.value   |   12151.385    |\n",
            "| bert.encoder.layer.0.attention.output.dense  |   11763.169    |\n",
            "|   bert.encoder.layer.0.intermediate.dense    |   63676.031    |\n",
            "|      bert.encoder.layer.0.output.dense       |   59927.969    |\n",
            "|  bert.encoder.layer.1.attention.self.query   |   17953.551    |\n",
            "|   bert.encoder.layer.1.attention.self.key    |   17884.939    |\n",
            "|  bert.encoder.layer.1.attention.self.value   |   11963.402    |\n",
            "| bert.encoder.layer.1.attention.output.dense  |   11522.994    |\n",
            "|   bert.encoder.layer.1.intermediate.dense    |   66579.664    |\n",
            "|      bert.encoder.layer.1.output.dense       |   62858.758    |\n",
            "|  bert.encoder.layer.2.attention.self.query   |   19654.184    |\n",
            "|   bert.encoder.layer.2.attention.self.key    |   19210.676    |\n",
            "|  bert.encoder.layer.2.attention.self.value   |   11774.834    |\n",
            "| bert.encoder.layer.2.attention.output.dense  |   11303.727    |\n",
            "|   bert.encoder.layer.2.intermediate.dense    |    67537.82    |\n",
            "|      bert.encoder.layer.2.output.dense       |   63650.957    |\n",
            "|  bert.encoder.layer.3.attention.self.query   |   18166.098    |\n",
            "|   bert.encoder.layer.3.attention.self.key    |   18056.039    |\n",
            "|  bert.encoder.layer.3.attention.self.value   |   13097.229    |\n",
            "| bert.encoder.layer.3.attention.output.dense  |   12130.952    |\n",
            "|   bert.encoder.layer.3.intermediate.dense    |   68518.266    |\n",
            "|      bert.encoder.layer.3.output.dense       |   64721.555    |\n",
            "|  bert.encoder.layer.4.attention.self.query   |   17855.461    |\n",
            "|   bert.encoder.layer.4.attention.self.key    |   17746.809    |\n",
            "|  bert.encoder.layer.4.attention.self.value   |   14720.646    |\n",
            "| bert.encoder.layer.4.attention.output.dense  |   13593.878    |\n",
            "|   bert.encoder.layer.4.intermediate.dense    |   68976.891    |\n",
            "|      bert.encoder.layer.4.output.dense       |   65581.297    |\n",
            "|  bert.encoder.layer.5.attention.self.query   |   18380.867    |\n",
            "|   bert.encoder.layer.5.attention.self.key    |   18347.285    |\n",
            "|  bert.encoder.layer.5.attention.self.value   |   14678.804    |\n",
            "| bert.encoder.layer.5.attention.output.dense  |   13869.033    |\n",
            "|   bert.encoder.layer.5.intermediate.dense    |   68757.195    |\n",
            "|      bert.encoder.layer.5.output.dense       |   64775.117    |\n",
            "|  bert.encoder.layer.6.attention.self.query   |   18274.145    |\n",
            "|   bert.encoder.layer.6.attention.self.key    |   18242.305    |\n",
            "|  bert.encoder.layer.6.attention.self.value   |   14342.701    |\n",
            "| bert.encoder.layer.6.attention.output.dense  |   13509.896    |\n",
            "|   bert.encoder.layer.6.intermediate.dense    |   69219.438    |\n",
            "|      bert.encoder.layer.6.output.dense       |   64324.273    |\n",
            "|  bert.encoder.layer.7.attention.self.query   |    18432.91    |\n",
            "|   bert.encoder.layer.7.attention.self.key    |   18477.527    |\n",
            "|  bert.encoder.layer.7.attention.self.value   |   13941.273    |\n",
            "| bert.encoder.layer.7.attention.output.dense  |   13360.079    |\n",
            "|   bert.encoder.layer.7.intermediate.dense    |   66811.141    |\n",
            "|      bert.encoder.layer.7.output.dense       |   62667.152    |\n",
            "|  bert.encoder.layer.8.attention.self.query   |    18732.66    |\n",
            "|   bert.encoder.layer.8.attention.self.key    |   18762.986    |\n",
            "|  bert.encoder.layer.8.attention.self.value   |   15015.994    |\n",
            "| bert.encoder.layer.8.attention.output.dense  |   14170.361    |\n",
            "|   bert.encoder.layer.8.intermediate.dense    |    66864.0     |\n",
            "|      bert.encoder.layer.8.output.dense       |   62639.734    |\n",
            "|  bert.encoder.layer.9.attention.self.query   |   19513.914    |\n",
            "|   bert.encoder.layer.9.attention.self.key    |   19465.609    |\n",
            "|  bert.encoder.layer.9.attention.self.value   |   14657.931    |\n",
            "| bert.encoder.layer.9.attention.output.dense  |   13891.488    |\n",
            "|   bert.encoder.layer.9.intermediate.dense    |   67440.031    |\n",
            "|      bert.encoder.layer.9.output.dense       |   64826.008    |\n",
            "|  bert.encoder.layer.10.attention.self.query  |   19491.426    |\n",
            "|   bert.encoder.layer.10.attention.self.key   |   19405.695    |\n",
            "|  bert.encoder.layer.10.attention.self.value  |   14956.383    |\n",
            "| bert.encoder.layer.10.attention.output.dense |   14114.705    |\n",
            "|   bert.encoder.layer.10.intermediate.dense   |   66560.859    |\n",
            "|      bert.encoder.layer.10.output.dense      |   63851.711    |\n",
            "|  bert.encoder.layer.11.attention.self.query  |   19205.025    |\n",
            "|   bert.encoder.layer.11.attention.self.key   |   18892.467    |\n",
            "|  bert.encoder.layer.11.attention.self.value  |    16616.34    |\n",
            "| bert.encoder.layer.11.attention.output.dense |   15663.816    |\n",
            "|   bert.encoder.layer.11.intermediate.dense   |   66860.617    |\n",
            "|      bert.encoder.layer.11.output.dense      |   60879.094    |\n",
            "|              bert.pooler.dense               |    12567.68    |\n",
            "|                  classifier                  |    1342.573    |\n",
            "|             Total Sum of Weights             |  2355265.292   |\n",
            "+----------------------------------------------+----------------+\n"
          ]
        }
      ],
      "source": [
        "# Pruned model summary\n",
        "pruned_bert.evaluate(eval_args)\n",
        "pruned_bert.print_model_size()\n",
        "pruned_bert.print_model()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "4e39dea2",
      "metadata": {
        "id": "4e39dea2"
      },
      "source": [
        "## b. Model 1 Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": 19,
      "id": "KVcS5ihuGnAV",
      "metadata": {
        "id": "KVcS5ihuGnAV"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.seq_relationship.bias', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.transform.dense.weight', 'cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.predictions.transform.dense.bias']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.weight', 'classifier.bias']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "Loading cached processed dataset at C:\\Users\\levan\\.cache\\huggingface\\datasets\\csv\\default-7b22cbae0e9605fe\\0.0.0\\eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d\\cache-eb12f0a8feddc4e7.arrow\n",
            "Loading cached processed dataset at C:\\Users\\levan\\.cache\\huggingface\\datasets\\csv\\default-7b22cbae0e9605fe\\0.0.0\\eea64c71ca8b46dd3f537ed218fc9bf495d5707789152eb2764f5c78fa66d59d\\cache-6aaa676ee15515d9.arrow\n",
            "c:\\Python310\\lib\\site-packages\\transformers\\optimization.py:411: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n",
            "Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Logging into wandb.ai. (Learn how to deploy a W&B server locally: https://wandb.me/wandb-server)\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: You can find your API key in your browser here: https://wandb.ai/authorize\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Paste an API key from your profile and hit enter, or press ctrl+c to quit:\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: C:\\Users\\levan/.netrc\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.4"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>c:\\Users\\levan\\Documents\\מסמכים של ניר\\אוניברסיטת תל אביב\\2022-2023\\נושאים מתקדמים בלמידה עמוקה\\פרויקט\\news-dl-project\\wandb\\run-20230701_212247-9w5yxqx7</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/nirlevanon/huggingface/runs/9w5yxqx7' target=\"_blank\">visionary-star-1</a></strong> to <a href='https://wandb.ai/nirlevanon/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/nirlevanon/huggingface' target=\"_blank\">https://wandb.ai/nirlevanon/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/nirlevanon/huggingface/runs/9w5yxqx7' target=\"_blank\">https://wandb.ai/nirlevanon/huggingface/runs/9w5yxqx7</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "  0%|          | 56/92985 [07:28<254:17:16,  9.85s/it]"
          ]
        },
        {
          "ename": "KeyboardInterrupt",
          "evalue": "",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[1;32mc:\\Users\\levan\\Documents\\מסמכים של ניר\\אוניברסיטת תל אביב\\2022-2023\\נושאים מתקדמים בלמידה עמוקה\\פרויקט\\news-dl-project\\Model_final.ipynb Cell 31\u001b[0m in \u001b[0;36m1\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=6'>7</a>\u001b[0m \u001b[39m# Train the quantized model for additional epochs\u001b[39;00m\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=7'>8</a>\u001b[0m train_args \u001b[39m=\u001b[39m TrainingArguments(\n\u001b[0;32m      <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=8'>9</a>\u001b[0m     output_dir\u001b[39m=\u001b[39moutput_dir,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=9'>10</a>\u001b[0m     overwrite_output_dir\u001b[39m=\u001b[39m\u001b[39mTrue\u001b[39;00m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=13'>14</a>\u001b[0m     save_strategy\u001b[39m=\u001b[39m\u001b[39m'\u001b[39m\u001b[39mno\u001b[39m\u001b[39m'\u001b[39m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=14'>15</a>\u001b[0m )\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=16'>17</a>\u001b[0m quantization_bert\u001b[39m.\u001b[39;49mtrain(train_args)\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=18'>19</a>\u001b[0m \u001b[39m# Save the trained quantized model's state dict\u001b[39;00m\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m quantization_bert\u001b[39m.\u001b[39msave_model(\u001b[39m'\u001b[39m\u001b[39mquantized_model_trained.pt\u001b[39m\u001b[39m'\u001b[39m)\n",
            "\u001b[1;32mc:\\Users\\levan\\Documents\\מסמכים של ניר\\אוניברסיטת תל אביב\\2022-2023\\נושאים מתקדמים בלמידה עמוקה\\פרויקט\\news-dl-project\\Model_final.ipynb Cell 31\u001b[0m in \u001b[0;36m2\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=19'>20</a>\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mtrain\u001b[39m(\u001b[39mself\u001b[39m, train_args):\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=20'>21</a>\u001b[0m     trainer \u001b[39m=\u001b[39m Trainer(\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=21'>22</a>\u001b[0m         model\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=22'>23</a>\u001b[0m         args\u001b[39m=\u001b[39mtrain_args,\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=23'>24</a>\u001b[0m         train_dataset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenized_dataset[\u001b[39m'\u001b[39m\u001b[39mtrain\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=24'>25</a>\u001b[0m         eval_dataset\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mtokenized_dataset[\u001b[39m'\u001b[39m\u001b[39mtest\u001b[39m\u001b[39m'\u001b[39m],\n\u001b[0;32m     <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=25'>26</a>\u001b[0m         compute_metrics\u001b[39m=\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmetric_fn)\n\u001b[1;32m---> <a href='vscode-notebook-cell:/c%3A/Users/levan/Documents/%D7%9E%D7%A1%D7%9E%D7%9B%D7%99%D7%9D%20%D7%A9%D7%9C%20%D7%A0%D7%99%D7%A8/%D7%90%D7%95%D7%A0%D7%99%D7%91%D7%A8%D7%A1%D7%99%D7%98%D7%AA%20%D7%AA%D7%9C%20%D7%90%D7%91%D7%99%D7%91/2022-2023/%D7%A0%D7%95%D7%A9%D7%90%D7%99%D7%9D%20%D7%9E%D7%AA%D7%A7%D7%93%D7%9E%D7%99%D7%9D%20%D7%91%D7%9C%D7%9E%D7%99%D7%93%D7%94%20%D7%A2%D7%9E%D7%95%D7%A7%D7%94/%D7%A4%D7%A8%D7%95%D7%99%D7%A7%D7%98/news-dl-project/Model_final.ipynb#X43sZmlsZQ%3D%3D?line=27'>28</a>\u001b[0m     trainer\u001b[39m.\u001b[39;49mtrain()\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:1645\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1640\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel_wrapped \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmodel\n\u001b[0;32m   1642\u001b[0m inner_training_loop \u001b[39m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1643\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_inner_training_loop, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_train_batch_size, args\u001b[39m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1644\u001b[0m )\n\u001b[1;32m-> 1645\u001b[0m \u001b[39mreturn\u001b[39;00m inner_training_loop(\n\u001b[0;32m   1646\u001b[0m     args\u001b[39m=\u001b[39;49margs,\n\u001b[0;32m   1647\u001b[0m     resume_from_checkpoint\u001b[39m=\u001b[39;49mresume_from_checkpoint,\n\u001b[0;32m   1648\u001b[0m     trial\u001b[39m=\u001b[39;49mtrial,\n\u001b[0;32m   1649\u001b[0m     ignore_keys_for_eval\u001b[39m=\u001b[39;49mignore_keys_for_eval,\n\u001b[0;32m   1650\u001b[0m )\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:1938\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   1935\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcallback_handler\u001b[39m.\u001b[39mon_step_begin(args, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcontrol)\n\u001b[0;32m   1937\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39maccelerator\u001b[39m.\u001b[39maccumulate(model):\n\u001b[1;32m-> 1938\u001b[0m     tr_loss_step \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mtraining_step(model, inputs)\n\u001b[0;32m   1940\u001b[0m \u001b[39mif\u001b[39;00m (\n\u001b[0;32m   1941\u001b[0m     args\u001b[39m.\u001b[39mlogging_nan_inf_filter\n\u001b[0;32m   1942\u001b[0m     \u001b[39mand\u001b[39;00m \u001b[39mnot\u001b[39;00m is_torch_tpu_available()\n\u001b[0;32m   1943\u001b[0m     \u001b[39mand\u001b[39;00m (torch\u001b[39m.\u001b[39misnan(tr_loss_step) \u001b[39mor\u001b[39;00m torch\u001b[39m.\u001b[39misinf(tr_loss_step))\n\u001b[0;32m   1944\u001b[0m ):\n\u001b[0;32m   1945\u001b[0m     \u001b[39m# if loss is nan or inf simply add the average of previous logged losses\u001b[39;00m\n\u001b[0;32m   1946\u001b[0m     tr_loss \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m tr_loss \u001b[39m/\u001b[39m (\u001b[39m1\u001b[39m \u001b[39m+\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mstate\u001b[39m.\u001b[39mglobal_step \u001b[39m-\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_globalstep_last_logged)\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:2759\u001b[0m, in \u001b[0;36mTrainer.training_step\u001b[1;34m(self, model, inputs)\u001b[0m\n\u001b[0;32m   2756\u001b[0m     \u001b[39mreturn\u001b[39;00m loss_mb\u001b[39m.\u001b[39mreduce_mean()\u001b[39m.\u001b[39mdetach()\u001b[39m.\u001b[39mto(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mdevice)\n\u001b[0;32m   2758\u001b[0m \u001b[39mwith\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcompute_loss_context_manager():\n\u001b[1;32m-> 2759\u001b[0m     loss \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mcompute_loss(model, inputs)\n\u001b[0;32m   2761\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mn_gpu \u001b[39m>\u001b[39m \u001b[39m1\u001b[39m:\n\u001b[0;32m   2762\u001b[0m     loss \u001b[39m=\u001b[39m loss\u001b[39m.\u001b[39mmean()  \u001b[39m# mean() to average on multi-gpu parallel training\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\trainer.py:2784\u001b[0m, in \u001b[0;36mTrainer.compute_loss\u001b[1;34m(self, model, inputs, return_outputs)\u001b[0m\n\u001b[0;32m   2782\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[0;32m   2783\u001b[0m     labels \u001b[39m=\u001b[39m \u001b[39mNone\u001b[39;00m\n\u001b[1;32m-> 2784\u001b[0m outputs \u001b[39m=\u001b[39m model(\u001b[39m*\u001b[39m\u001b[39m*\u001b[39minputs)\n\u001b[0;32m   2785\u001b[0m \u001b[39m# Save past state if it exists\u001b[39;00m\n\u001b[0;32m   2786\u001b[0m \u001b[39m# TODO: this needs to be fixed and made cleaner later.\u001b[39;00m\n\u001b[0;32m   2787\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39margs\u001b[39m.\u001b[39mpast_index \u001b[39m>\u001b[39m\u001b[39m=\u001b[39m \u001b[39m0\u001b[39m:\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1562\u001b[0m, in \u001b[0;36mBertForSequenceClassification.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, labels, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1554\u001b[0m \u001b[39mr\u001b[39m\u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1555\u001b[0m \u001b[39mlabels (`torch.LongTensor` of shape `(batch_size,)`, *optional*):\u001b[39;00m\n\u001b[0;32m   1556\u001b[0m \u001b[39m    Labels for computing the sequence classification/regression loss. Indices should be in `[0, ...,\u001b[39;00m\n\u001b[0;32m   1557\u001b[0m \u001b[39m    config.num_labels - 1]`. If `config.num_labels == 1` a regression loss is computed (Mean-Square loss), If\u001b[39;00m\n\u001b[0;32m   1558\u001b[0m \u001b[39m    `config.num_labels > 1` a classification loss is computed (Cross-Entropy).\u001b[39;00m\n\u001b[0;32m   1559\u001b[0m \u001b[39m\"\"\"\u001b[39;00m\n\u001b[0;32m   1560\u001b[0m return_dict \u001b[39m=\u001b[39m return_dict \u001b[39mif\u001b[39;00m return_dict \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39muse_return_dict\n\u001b[1;32m-> 1562\u001b[0m outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbert(\n\u001b[0;32m   1563\u001b[0m     input_ids,\n\u001b[0;32m   1564\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mattention_mask,\n\u001b[0;32m   1565\u001b[0m     token_type_ids\u001b[39m=\u001b[39;49mtoken_type_ids,\n\u001b[0;32m   1566\u001b[0m     position_ids\u001b[39m=\u001b[39;49mposition_ids,\n\u001b[0;32m   1567\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1568\u001b[0m     inputs_embeds\u001b[39m=\u001b[39;49minputs_embeds,\n\u001b[0;32m   1569\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1570\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1571\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1572\u001b[0m )\n\u001b[0;32m   1574\u001b[0m pooled_output \u001b[39m=\u001b[39m outputs[\u001b[39m1\u001b[39m]\n\u001b[0;32m   1576\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(pooled_output)\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:1020\u001b[0m, in \u001b[0;36mBertModel.forward\u001b[1;34m(self, input_ids, attention_mask, token_type_ids, position_ids, head_mask, inputs_embeds, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m   1011\u001b[0m head_mask \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_head_mask(head_mask, \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mconfig\u001b[39m.\u001b[39mnum_hidden_layers)\n\u001b[0;32m   1013\u001b[0m embedding_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39membeddings(\n\u001b[0;32m   1014\u001b[0m     input_ids\u001b[39m=\u001b[39minput_ids,\n\u001b[0;32m   1015\u001b[0m     position_ids\u001b[39m=\u001b[39mposition_ids,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m   1018\u001b[0m     past_key_values_length\u001b[39m=\u001b[39mpast_key_values_length,\n\u001b[0;32m   1019\u001b[0m )\n\u001b[1;32m-> 1020\u001b[0m encoder_outputs \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mencoder(\n\u001b[0;32m   1021\u001b[0m     embedding_output,\n\u001b[0;32m   1022\u001b[0m     attention_mask\u001b[39m=\u001b[39;49mextended_attention_mask,\n\u001b[0;32m   1023\u001b[0m     head_mask\u001b[39m=\u001b[39;49mhead_mask,\n\u001b[0;32m   1024\u001b[0m     encoder_hidden_states\u001b[39m=\u001b[39;49mencoder_hidden_states,\n\u001b[0;32m   1025\u001b[0m     encoder_attention_mask\u001b[39m=\u001b[39;49mencoder_extended_attention_mask,\n\u001b[0;32m   1026\u001b[0m     past_key_values\u001b[39m=\u001b[39;49mpast_key_values,\n\u001b[0;32m   1027\u001b[0m     use_cache\u001b[39m=\u001b[39;49muse_cache,\n\u001b[0;32m   1028\u001b[0m     output_attentions\u001b[39m=\u001b[39;49moutput_attentions,\n\u001b[0;32m   1029\u001b[0m     output_hidden_states\u001b[39m=\u001b[39;49moutput_hidden_states,\n\u001b[0;32m   1030\u001b[0m     return_dict\u001b[39m=\u001b[39;49mreturn_dict,\n\u001b[0;32m   1031\u001b[0m )\n\u001b[0;32m   1032\u001b[0m sequence_output \u001b[39m=\u001b[39m encoder_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m   1033\u001b[0m pooled_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler(sequence_output) \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mpooler \u001b[39mis\u001b[39;00m \u001b[39mnot\u001b[39;00m \u001b[39mNone\u001b[39;00m \u001b[39melse\u001b[39;00m \u001b[39mNone\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:610\u001b[0m, in \u001b[0;36mBertEncoder.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_values, use_cache, output_attentions, output_hidden_states, return_dict)\u001b[0m\n\u001b[0;32m    601\u001b[0m     layer_outputs \u001b[39m=\u001b[39m torch\u001b[39m.\u001b[39mutils\u001b[39m.\u001b[39mcheckpoint\u001b[39m.\u001b[39mcheckpoint(\n\u001b[0;32m    602\u001b[0m         create_custom_forward(layer_module),\n\u001b[0;32m    603\u001b[0m         hidden_states,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    607\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    608\u001b[0m     )\n\u001b[0;32m    609\u001b[0m \u001b[39melse\u001b[39;00m:\n\u001b[1;32m--> 610\u001b[0m     layer_outputs \u001b[39m=\u001b[39m layer_module(\n\u001b[0;32m    611\u001b[0m         hidden_states,\n\u001b[0;32m    612\u001b[0m         attention_mask,\n\u001b[0;32m    613\u001b[0m         layer_head_mask,\n\u001b[0;32m    614\u001b[0m         encoder_hidden_states,\n\u001b[0;32m    615\u001b[0m         encoder_attention_mask,\n\u001b[0;32m    616\u001b[0m         past_key_value,\n\u001b[0;32m    617\u001b[0m         output_attentions,\n\u001b[0;32m    618\u001b[0m     )\n\u001b[0;32m    620\u001b[0m hidden_states \u001b[39m=\u001b[39m layer_outputs[\u001b[39m0\u001b[39m]\n\u001b[0;32m    621\u001b[0m \u001b[39mif\u001b[39;00m use_cache:\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:537\u001b[0m, in \u001b[0;36mBertLayer.forward\u001b[1;34m(self, hidden_states, attention_mask, head_mask, encoder_hidden_states, encoder_attention_mask, past_key_value, output_attentions)\u001b[0m\n\u001b[0;32m    534\u001b[0m     cross_attn_present_key_value \u001b[39m=\u001b[39m cross_attention_outputs[\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m]\n\u001b[0;32m    535\u001b[0m     present_key_value \u001b[39m=\u001b[39m present_key_value \u001b[39m+\u001b[39m cross_attn_present_key_value\n\u001b[1;32m--> 537\u001b[0m layer_output \u001b[39m=\u001b[39m apply_chunking_to_forward(\n\u001b[0;32m    538\u001b[0m     \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mfeed_forward_chunk, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mchunk_size_feed_forward, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mseq_len_dim, attention_output\n\u001b[0;32m    539\u001b[0m )\n\u001b[0;32m    540\u001b[0m outputs \u001b[39m=\u001b[39m (layer_output,) \u001b[39m+\u001b[39m outputs\n\u001b[0;32m    542\u001b[0m \u001b[39m# if decoder, return the attn key/values as the last output\u001b[39;00m\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\pytorch_utils.py:237\u001b[0m, in \u001b[0;36mapply_chunking_to_forward\u001b[1;34m(forward_fn, chunk_size, chunk_dim, *input_tensors)\u001b[0m\n\u001b[0;32m    234\u001b[0m     \u001b[39m# concatenate output at same dimension\u001b[39;00m\n\u001b[0;32m    235\u001b[0m     \u001b[39mreturn\u001b[39;00m torch\u001b[39m.\u001b[39mcat(output_chunks, dim\u001b[39m=\u001b[39mchunk_dim)\n\u001b[1;32m--> 237\u001b[0m \u001b[39mreturn\u001b[39;00m forward_fn(\u001b[39m*\u001b[39;49minput_tensors)\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:550\u001b[0m, in \u001b[0;36mBertLayer.feed_forward_chunk\u001b[1;34m(self, attention_output)\u001b[0m\n\u001b[0;32m    548\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mfeed_forward_chunk\u001b[39m(\u001b[39mself\u001b[39m, attention_output):\n\u001b[0;32m    549\u001b[0m     intermediate_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mintermediate(attention_output)\n\u001b[1;32m--> 550\u001b[0m     layer_output \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49moutput(intermediate_output, attention_output)\n\u001b[0;32m    551\u001b[0m     \u001b[39mreturn\u001b[39;00m layer_output\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\transformers\\models\\bert\\modeling_bert.py:462\u001b[0m, in \u001b[0;36mBertOutput.forward\u001b[1;34m(self, hidden_states, input_tensor)\u001b[0m\n\u001b[0;32m    461\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, hidden_states: torch\u001b[39m.\u001b[39mTensor, input_tensor: torch\u001b[39m.\u001b[39mTensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m torch\u001b[39m.\u001b[39mTensor:\n\u001b[1;32m--> 462\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mdense(hidden_states)\n\u001b[0;32m    463\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mdropout(hidden_states)\n\u001b[0;32m    464\u001b[0m     hidden_states \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mLayerNorm(hidden_states \u001b[39m+\u001b[39m input_tensor)\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\module.py:1190\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[1;34m(self, *input, **kwargs)\u001b[0m\n\u001b[0;32m   1186\u001b[0m \u001b[39m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[0;32m   1187\u001b[0m \u001b[39m# this function, and just call forward.\u001b[39;00m\n\u001b[0;32m   1188\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mnot\u001b[39;00m (\u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_backward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_hooks \u001b[39mor\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39m_forward_pre_hooks \u001b[39mor\u001b[39;00m _global_backward_hooks\n\u001b[0;32m   1189\u001b[0m         \u001b[39mor\u001b[39;00m _global_forward_hooks \u001b[39mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[1;32m-> 1190\u001b[0m     \u001b[39mreturn\u001b[39;00m forward_call(\u001b[39m*\u001b[39m\u001b[39minput\u001b[39m, \u001b[39m*\u001b[39m\u001b[39m*\u001b[39mkwargs)\n\u001b[0;32m   1191\u001b[0m \u001b[39m# Do not call functions when jit is used\u001b[39;00m\n\u001b[0;32m   1192\u001b[0m full_backward_hooks, non_full_backward_hooks \u001b[39m=\u001b[39m [], []\n",
            "File \u001b[1;32mc:\\Python310\\lib\\site-packages\\torch\\nn\\modules\\linear.py:114\u001b[0m, in \u001b[0;36mLinear.forward\u001b[1;34m(self, input)\u001b[0m\n\u001b[0;32m    113\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mforward\u001b[39m(\u001b[39mself\u001b[39m, \u001b[39minput\u001b[39m: Tensor) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m Tensor:\n\u001b[1;32m--> 114\u001b[0m     \u001b[39mreturn\u001b[39;00m F\u001b[39m.\u001b[39;49mlinear(\u001b[39minput\u001b[39;49m, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mweight, \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mbias)\n",
            "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ],
      "source": [
        "# Load the quantized model\n",
        "\n",
        "quantization_bert = Model(model_name_1, datasets, num_of_classes)\n",
        "quantization_bert.tokenize(token_args)\n",
        "quantization_bert.model.load_state_dict(torch.load(Path(home_dir, 'bert_regular.pt'), map_location=torch.device(device)))\n",
        "\n",
        "# Train the quantized model for additional epochs\n",
        "train_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_strategy='epoch',\n",
        "    num_train_epochs=5,  # Specify the desired number of additional epochs\n",
        "    save_strategy='no'\n",
        ")\n",
        "\n",
        "quantization_bert.train(train_args)\n",
        "\n",
        "# Save the trained quantized model's state dict\n",
        "quantization_bert.save_model('quantized_model_trained.pt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "755148ef",
      "metadata": {
        "id": "755148ef"
      },
      "source": [
        "## c. Model 1 Knowledge-Distillation"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "d06f4d7b",
      "metadata": {
        "id": "d06f4d7b"
      },
      "source": [
        "## Help function for model distilation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d813f13d",
      "metadata": {
        "id": "d813f13d"
      },
      "outputs": [],
      "source": [
        "class Student_Calssifier(nn.Module):\n",
        "    def __init__(self, vocab_size, embedding_dim, hidden_dim, num_layers, num_heads, num_classes, dropout=0.1):\n",
        "        super(Student_Calssifier, self).__init__()\n",
        "\n",
        "        self.embedding = nn.Embedding(vocab_size, embedding_dim)\n",
        "        self.encoder_layers = nn.TransformerEncoderLayer(embedding_dim, num_heads, hidden_dim, dropout)\n",
        "        self.transformer_encoder = nn.TransformerEncoder(self.encoder_layers, num_layers)\n",
        "        self.fc = nn.Linear(embedding_dim, num_classes)\n",
        "\n",
        "    def forward(self, x):\n",
        "        embedded = self.embedding(x)\n",
        "        embedded = embedded.permute(1, 0, 2)  # Adjust shape for transformer\n",
        "        transformer_output = self.transformer_encoder(embedded)\n",
        "        transformer_output = transformer_output.permute(1, 0, 2)  # Adjust shape back to (batch_size, seq_length, embedding_dim)\n",
        "        logits = self.fc(transformer_output[:, -1, :])  # Take the last hidden state\n",
        "        return logits.squeeze()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "479338d3",
      "metadata": {
        "id": "479338d3"
      },
      "outputs": [],
      "source": [
        "def kl_training(Student, Teacher, epochs, alpha_teacher, lr, loss_func, data):\n",
        "    optimizer = torch.optim.Adam(Student.parameters(), lr=lr)\n",
        "    count = 0\n",
        "    optimizer.zero_grad()\n",
        "    losses = list()\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        for index, i in enumerate(data):\n",
        "            loss_e = 0\n",
        "            count += 1\n",
        "            inputs = i['input_ids']\n",
        "            y_index = i['labels']\n",
        "            attention_mask = torch.tensor(i['attention_mask']).to(device)\n",
        "            inputs = torch.tensor(inputs).to(device)\n",
        "            # Calculate loss with respect to teacher logits\n",
        "            teacher_logits = Teacher(input_ids=inputs.unsqueeze(-1), attention_mask=attention_mask.unsqueeze(-1))[0]\n",
        "            outputs = Student(inputs.unsqueeze(-1))\n",
        "            loss_teacher = loss_func(outputs, teacher_logits)\n",
        "\n",
        "            # Calculate loss with respect to ground truth\n",
        "            temp_array = np.zeros(num_classes)\n",
        "            temp_array[y_index] = 1\n",
        "            target = torch.from_numpy(temp_array).unsqueeze(0).to(device)\n",
        "            loss_ground_truth = loss_func(outputs, target)\n",
        "\n",
        "            loss = alpha_teacher * loss_teacher + (1 - alpha_teacher) * loss_ground_truth\n",
        "            loss_e += loss.item()\n",
        "            if count == 64:\n",
        "                loss.backward()\n",
        "                optimizer.step()\n",
        "                optimizer.zero_grad()\n",
        "                count = 0\n",
        "            if index%1000 == 0:\n",
        "                print(index)\n",
        "        losses.append(loss_e)\n",
        "        print(f'Epoch {epoch} was finished with train loss = {loss_e}')\n",
        "    return Student"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f161141b",
      "metadata": {
        "id": "f161141b"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "num_heads = 2\n",
        "num_classes = 62\n",
        "epochs = 5\n",
        "alpha_teacher = 0.8\n",
        "lr=0.001\n",
        "loss_func = nn.MSELoss()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "d2e5c7ce",
      "metadata": {
        "id": "d2e5c7ce",
        "outputId": "014aaf48-7d69-46e9-f940-11974f45459e",
        "scrolled": true
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "Some weights of the model checkpoint at bert-base-uncased were not used when initializing BertForSequenceClassification: ['cls.predictions.bias', 'cls.predictions.transform.LayerNorm.bias', 'cls.seq_relationship.bias', 'cls.predictions.transform.dense.bias', 'cls.predictions.transform.dense.weight', 'cls.predictions.transform.LayerNorm.weight', 'cls.seq_relationship.weight', 'cls.predictions.decoder.weight']\n",
            "- This IS expected if you are initializing BertForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing BertForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of BertForSequenceClassification were not initialized from the model checkpoint at bert-base-uncased and are newly initialized: ['classifier.bias', 'classifier.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\urllib3\\connectionpool.py:1045: InsecureRequestWarning: Unverified HTTPS request is being made to host 'huggingface.co'. Adding certificate verification is strongly advised. See: https://urllib3.readthedocs.io/en/1.26.x/advanced-usage.html#ssl-warnings\n",
            "  warnings.warn(\n",
            "Loading cached processed dataset at C:\\Users\\liyag\\.cache\\huggingface\\datasets\\csv\\default-f0e020e5f3a5fb7b\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-d0c596d113269e70.arrow\n",
            "Loading cached processed dataset at C:\\Users\\liyag\\.cache\\huggingface\\datasets\\csv\\default-f0e020e5f3a5fb7b\\0.0.0\\6954658bab30a358235fa864b05cf819af0e179325c740e4bc853bcc7ec513e1\\cache-58ee211d4801a52d.arrow\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "<All keys matched successfully>"
            ]
          },
          "execution_count": 36,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "kl_bert = Model(model_name_1, datasets, num_of_classes)\n",
        "kl_bert.tokenize(token_args)\n",
        "\n",
        "# Load the model's best parameters\n",
        "kl_bert.model.load_state_dict(torch.load(Path(home_dir, 'bert_regular.pt')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "1fc4139f",
      "metadata": {
        "id": "1fc4139f",
        "outputId": "84061ec8-65c4-4cb3-e693-3d0d6e157076"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\liyag\\AppData\\Local\\Temp\\ipykernel_21452\\2360740035.py:13: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  attention_mask = torch.tensor(i['attention_mask']).to(device)\n",
            "C:\\Users\\liyag\\AppData\\Local\\Temp\\ipykernel_21452\\2360740035.py:14: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  inputs = torch.tensor(inputs).to(device)\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\nn\\modules\\loss.py:536: UserWarning: Using a target size (torch.Size([1, 62])) that is different to the input size (torch.Size([64, 62])). This will likely lead to incorrect results due to broadcasting. Please ensure they have the same size.\n",
            "  return F.mse_loss(input, target, reduction=self.reduction)\n"
          ]
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "0\n"
          ]
        },
        {
          "ename": "RuntimeError",
          "evalue": "Found dtype Double but expected Float",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[37], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m bert_student \u001b[38;5;241m=\u001b[39m \u001b[43mkl_bert\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mkl\u001b[49m\u001b[43m(\u001b[49m\u001b[43mhidden_dim\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_layers\u001b[49m\u001b[43m,\u001b[49m\u001b[43mnum_heads\u001b[49m\u001b[43m,\u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_teacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m)\u001b[49m\n",
            "Cell \u001b[1;32mIn[27], line 116\u001b[0m, in \u001b[0;36mModel.kl\u001b[1;34m(self, hidden_dim, num_layers, num_heads, epochs, alpha_teacher, lr, loss_func)\u001b[0m\n\u001b[0;32m    113\u001b[0m Student\u001b[38;5;241m.\u001b[39mto(device)\n\u001b[0;32m    114\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m]\n\u001b[1;32m--> 116\u001b[0m Student \u001b[38;5;241m=\u001b[39m \u001b[43mkl_training\u001b[49m\u001b[43m(\u001b[49m\u001b[43mStudent\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mTeacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43malpha_teacher\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mloss_func\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdata\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    117\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m Student\n",
            "Cell \u001b[1;32mIn[34], line 29\u001b[0m, in \u001b[0;36mkl_training\u001b[1;34m(Student, Teacher, epochs, alpha_teacher, lr, loss_func, data)\u001b[0m\n\u001b[0;32m     27\u001b[0m loss_e \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m loss\u001b[38;5;241m.\u001b[39mitem()\n\u001b[0;32m     28\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m count \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m64\u001b[39m:\n\u001b[1;32m---> 29\u001b[0m     \u001b[43mloss\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     30\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mstep()\n\u001b[0;32m     31\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\_tensor.py:487\u001b[0m, in \u001b[0;36mTensor.backward\u001b[1;34m(self, gradient, retain_graph, create_graph, inputs)\u001b[0m\n\u001b[0;32m    477\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m has_torch_function_unary(\u001b[38;5;28mself\u001b[39m):\n\u001b[0;32m    478\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m handle_torch_function(\n\u001b[0;32m    479\u001b[0m         Tensor\u001b[38;5;241m.\u001b[39mbackward,\n\u001b[0;32m    480\u001b[0m         (\u001b[38;5;28mself\u001b[39m,),\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m    485\u001b[0m         inputs\u001b[38;5;241m=\u001b[39minputs,\n\u001b[0;32m    486\u001b[0m     )\n\u001b[1;32m--> 487\u001b[0m \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mautograd\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbackward\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m    488\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgradient\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43minputs\u001b[49m\n\u001b[0;32m    489\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\autograd\\__init__.py:200\u001b[0m, in \u001b[0;36mbackward\u001b[1;34m(tensors, grad_tensors, retain_graph, create_graph, grad_variables, inputs)\u001b[0m\n\u001b[0;32m    195\u001b[0m     retain_graph \u001b[38;5;241m=\u001b[39m create_graph\n\u001b[0;32m    197\u001b[0m \u001b[38;5;66;03m# The reason we repeat same the comment below is that\u001b[39;00m\n\u001b[0;32m    198\u001b[0m \u001b[38;5;66;03m# some Python versions print out the first line of a multi-line function\u001b[39;00m\n\u001b[0;32m    199\u001b[0m \u001b[38;5;66;03m# calls in the traceback and some print out the last line\u001b[39;00m\n\u001b[1;32m--> 200\u001b[0m \u001b[43mVariable\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execution_engine\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mrun_backward\u001b[49m\u001b[43m(\u001b[49m\u001b[43m  \u001b[49m\u001b[38;5;66;43;03m# Calls into the C++ engine to run the backward pass\u001b[39;49;00m\n\u001b[0;32m    201\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtensors\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mgrad_tensors_\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mretain_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcreate_graph\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43minputs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m    202\u001b[0m \u001b[43m    \u001b[49m\u001b[43mallow_unreachable\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maccumulate_grad\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;28;43;01mTrue\u001b[39;49;00m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: Found dtype Double but expected Float"
          ]
        }
      ],
      "source": [
        "bert_student = kl_bert.kl(hidden_dim,num_layers,num_heads,epochs, alpha_teacher, lr, loss_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a5ff640",
      "metadata": {
        "id": "7a5ff640"
      },
      "outputs": [],
      "source": [
        "student_model = Model(model_name_1, datasets, num_of_classes)\n",
        "student_model.tokenize(token_args)\n",
        "student_model.model = bert_student\n",
        "\n",
        "# save model's state dict\n",
        "student_model.save_model('bert_student')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "cf6c8127",
      "metadata": {
        "id": "cf6c8127"
      },
      "outputs": [],
      "source": [
        "# Student model summary\n",
        "student_model.evaluate(eval_args)\n",
        "student_model.print_model_size()\n",
        "student_model.print_model()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "1dcb5d83",
      "metadata": {
        "id": "1dcb5d83"
      },
      "source": [
        "# <u>Model 2 - ROBERTA Base<u>\n",
        "## Model 2 Definition and Tokenization:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "40f98e88",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "96599ca978fa4164b829147bab1471e0",
            "aea41452b271415e8ccda0242c15f0a5",
            "2018f1a574ba4502a46b10b4b4d16799",
            ""
          ]
        },
        "id": "40f98e88",
        "outputId": "6ef39e00-2089-414a-9857-bc51490bc72c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "Some weights of the model checkpoint at roberta-base were not used when initializing RobertaForSequenceClassification: ['lm_head.decoder.weight', 'lm_head.dense.bias', 'lm_head.layer_norm.weight', 'roberta.pooler.dense.weight', 'lm_head.dense.weight', 'lm_head.bias', 'roberta.pooler.dense.bias', 'lm_head.layer_norm.bias']\n",
            "- This IS expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model trained on another task or with another architecture (e.g. initializing a BertForSequenceClassification model from a BertForPreTraining model).\n",
            "- This IS NOT expected if you are initializing RobertaForSequenceClassification from the checkpoint of a model that you expect to be exactly identical (initializing a BertForSequenceClassification model from a BertForSequenceClassification model).\n",
            "Some weights of RobertaForSequenceClassification were not initialized from the model checkpoint at roberta-base and are newly initialized: ['classifier.out_proj.bias', 'classifier.dense.bias', 'classifier.out_proj.weight', 'classifier.dense.weight']\n",
            "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "96599ca978fa4164b829147bab1471e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/vocab.json:   0%|          | 0.00/899k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\huggingface_hub\\file_download.py:133: UserWarning: `huggingface_hub` cache-system uses symlinks by default to efficiently store duplicated files but your machine does not support them in C:\\Users\\liyag\\.cache\\huggingface\\hub. Caching files will still work but in a degraded version that might require more space on your disk. This warning can be disabled by setting the `HF_HUB_DISABLE_SYMLINKS_WARNING` environment variable. For more details, see https://huggingface.co/docs/huggingface_hub/how-to-cache#limitations.\n",
            "To support symlinks on Windows, you either need to activate Developer Mode or to run Python as an administrator. In order to see activate developer mode, see this article: https://docs.microsoft.com/en-us/windows/apps/get-started/enable-your-device-for-development\n",
            "  warnings.warn(message)\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "aea41452b271415e8ccda0242c15f0a5",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)olve/main/merges.txt:   0%|          | 0.00/456k [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "2018f1a574ba4502a46b10b4b4d16799",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Downloading (…)/main/tokenizer.json:   0%|          | 0.00/1.36M [00:00<?, ?B/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/148800 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "Map:   0%|          | 0/37200 [00:00<?, ? examples/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "model_name_2 = \"roberta-base\"\n",
        "num_of_classes = 62\n",
        "roberta = Model(model_name_2, datasets, num_of_classes)\n",
        "token_args = {\"max_length\": 64, \"truncation\": True, \"padding\": \"max_length\"}\n",
        "roberta.tokenize(token_args)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "ce701349",
      "metadata": {
        "id": "ce701349"
      },
      "source": [
        "## Model 2 Hyperparameter Search"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "a36f0ba0",
      "metadata": {
        "id": "a36f0ba0",
        "outputId": "06a06202-aeb0-4cbc-bd82-31bc023f518c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mliyag\u001b[0m (\u001b[33mdelta_lxr\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230615_182357-iezfpwqf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/DeepLearning/runs/iezfpwqf' target=\"_blank\">dutiful-bee-19</a></strong> to <a href='https://wandb.ai/delta_lxr/DeepLearning' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/DeepLearning' target=\"_blank\">https://wandb.ai/delta_lxr/DeepLearning</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/DeepLearning/runs/iezfpwqf' target=\"_blank\">https://wandb.ai/delta_lxr/DeepLearning/runs/iezfpwqf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "wandb.init(project=\"DeepLearning\")\n",
        "\n",
        "train_args = TrainingArguments(output_dir=output_dir,\n",
        "                             overwrite_output_dir=True,\n",
        "                             greater_is_better=True,\n",
        "                             evaluation_strategy='epoch',\n",
        "                             do_train=True,\n",
        "                             logging_strategy='epoch',\n",
        "                             save_strategy='no',\n",
        "                             report_to='wandb')\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "59cde0f6",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "e1e14a7204844461837cc5b017e6e5a0",
            "f8d5dab0a76f421bad2c4383af038174",
            "b4670a558589426f8798ee16bd686e59",
            "f891c6db995a496381b7c5aaccf7a789",
            "6ec92216540540b4b9ebb297e68c78e8",
            "ce11156f4c5c47b29a87d36698ee8ea3",
            "32eaa37f3f41414fa129b366bd9e4d60",
            "ce57f54344d9421da2cfa934d9eb58d8",
            "db4f92eaab564785b72e01ab2013a3ba",
            "ff6127c406ab4e9295f38101f7ced412",
            "a5b5e406e7234ec592ec527afc58ba0d",
            "f6fa0316e8b943af8b3e34a45ce364e0",
            "33aa3ad51e6849f2a4da1d051dd4c74b",
            "6b3936a8707c47bc8043b15f8b017576",
            "502473480a734dda857fdcb1f207d503"
          ]
        },
        "id": "59cde0f6",
        "outputId": "4e3abe89-c546-4df8-a983-74b54723062c"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:359: FutureWarning: `Trainer` requires either a `model` or `model_init` argument, but not both. `model_init` will overwrite your model when calling the `train` method. This will become a fatal error in the next release.\n",
            "  warnings.warn(\n",
            "\u001b[32m[I 2023-06-15 18:24:12,873]\u001b[0m A new study created in memory with name: no-name-341e8ab4-bff7-482f-a97a-b3ee3a305697\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "e1e14a7204844461837cc5b017e6e5a0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">dutiful-bee-19</strong> at: <a href='https://wandb.ai/delta_lxr/DeepLearning/runs/iezfpwqf' target=\"_blank\">https://wandb.ai/delta_lxr/DeepLearning/runs/iezfpwqf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230615_182357-iezfpwqf\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f8d5dab0a76f421bad2c4383af038174",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666672124, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230615_182417-ckb9k4yb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/ckb9k4yb' target=\"_blank\">balmy-totem-104</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/ckb9k4yb' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/ckb9k4yb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='65100' max='65100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [65100/65100 1:09:03, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>3.494300</td>\n",
              "      <td>2.952349</td>\n",
              "      <td>0.237611</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.867600</td>\n",
              "      <td>2.599500</td>\n",
              "      <td>0.314514</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.624600</td>\n",
              "      <td>2.439085</td>\n",
              "      <td>0.354110</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.487900</td>\n",
              "      <td>2.346071</td>\n",
              "      <td>0.368190</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.401000</td>\n",
              "      <td>2.289408</td>\n",
              "      <td>0.385501</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>2.351900</td>\n",
              "      <td>2.260303</td>\n",
              "      <td>0.389990</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>2.325400</td>\n",
              "      <td>2.249121</td>\n",
              "      <td>0.392198</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-15 19:33:30,864]\u001b[0m Trial 0 finished with value: 0.392197507549085 and parameters: {'learning_rate': 1.0906450785722703e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 1, 'warmup_steps': 298, 'weight_decay': 0.02419408513272277, 'per_device_eval_batch_size': 16}. Best is trial 0 with value: 0.392197507549085.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "b4670a558589426f8798ee16bd686e59",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▆▇███</td></tr><tr><td>eval/loss</td><td>█▄▃▂▁▁▁</td></tr><tr><td>eval/runtime</td><td>▂▂▁▅█▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▇▇█▄▁▄█</td></tr><tr><td>eval/steps_per_second</td><td>▇▇█▄▁▄█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▄▃▂▁▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.3922</td></tr><tr><td>eval/loss</td><td>2.24912</td></tr><tr><td>eval/runtime</td><td>30.2551</td></tr><tr><td>eval/samples_per_second</td><td>1229.544</td></tr><tr><td>eval/steps_per_second</td><td>76.847</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>65100</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>2.3254</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>2.6504</td></tr><tr><td>train/train_runtime</td><td>4157.9793</td></tr><tr><td>train/train_samples_per_second</td><td>250.506</td></tr><tr><td>train/train_steps_per_second</td><td>15.657</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">balmy-totem-104</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/ckb9k4yb' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/ckb9k4yb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230615_182417-ckb9k4yb\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f891c6db995a496381b7c5aaccf7a789",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230615_193336-ua0w67kn</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/ua0w67kn' target=\"_blank\">robust-eon-105</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/ua0w67kn' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/ua0w67kn</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21700' max='21700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21700/21700 51:50, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>2.277000</td>\n",
              "      <td>2.171113</td>\n",
              "      <td>0.414297</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>2.167500</td>\n",
              "      <td>2.098306</td>\n",
              "      <td>0.436412</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>2.100200</td>\n",
              "      <td>2.053305</td>\n",
              "      <td>0.450292</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>2.051400</td>\n",
              "      <td>2.018600</td>\n",
              "      <td>0.454959</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>2.013100</td>\n",
              "      <td>1.991603</td>\n",
              "      <td>0.466067</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.994400</td>\n",
              "      <td>1.978873</td>\n",
              "      <td>0.467591</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.984600</td>\n",
              "      <td>1.972843</td>\n",
              "      <td>0.470023</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-15 20:25:36,941]\u001b[0m Trial 1 finished with value: 0.4700229900169208 and parameters: {'learning_rate': 1.7866264609909754e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 3, 'warmup_steps': 161, 'weight_decay': 0.05205396216297007, 'per_device_eval_batch_size': 16}. Best is trial 1 with value: 0.4700229900169208.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6ec92216540540b4b9ebb297e68c78e8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▆▆███</td></tr><tr><td>eval/loss</td><td>█▅▄▃▂▁▁</td></tr><tr><td>eval/runtime</td><td>▁▅▆▅▅█▆</td></tr><tr><td>eval/samples_per_second</td><td>█▄▃▄▄▁▃</td></tr><tr><td>eval/steps_per_second</td><td>█▄▃▄▄▁▃</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▄▃▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.47002</td></tr><tr><td>eval/loss</td><td>1.97284</td></tr><tr><td>eval/runtime</td><td>30.4985</td></tr><tr><td>eval/samples_per_second</td><td>1219.732</td></tr><tr><td>eval/steps_per_second</td><td>76.233</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>21700</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.9846</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>2.08402</td></tr><tr><td>train/train_runtime</td><td>3126.0649</td></tr><tr><td>train/train_samples_per_second</td><td>333.198</td></tr><tr><td>train/train_steps_per_second</td><td>6.942</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">robust-eon-105</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/ua0w67kn' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/ua0w67kn</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230615_193336-ua0w67kn\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230615_202542-bwifcff0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/bwifcff0' target=\"_blank\">trim-flower-106</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/bwifcff0' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/bwifcff0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21700' max='21700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21700/21700 51:49, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.936300</td>\n",
              "      <td>1.867773</td>\n",
              "      <td>0.499651</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.731800</td>\n",
              "      <td>1.744479</td>\n",
              "      <td>0.537473</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>1.588100</td>\n",
              "      <td>1.664634</td>\n",
              "      <td>0.560429</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>1.479500</td>\n",
              "      <td>1.629033</td>\n",
              "      <td>0.569478</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>1.394500</td>\n",
              "      <td>1.585030</td>\n",
              "      <td>0.583015</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>1.334600</td>\n",
              "      <td>1.562527</td>\n",
              "      <td>0.590499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>1.297400</td>\n",
              "      <td>1.553933</td>\n",
              "      <td>0.592215</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-15 21:17:39,893]\u001b[0m Trial 2 finished with value: 0.5922154691110901 and parameters: {'learning_rate': 8.42671234623641e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 3, 'warmup_steps': 42, 'weight_decay': 0.0964488402496755, 'per_device_eval_batch_size': 16}. Best is trial 2 with value: 0.5922154691110901.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce11156f4c5c47b29a87d36698ee8ea3",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▄▆▆▇██</td></tr><tr><td>eval/loss</td><td>█▅▃▃▂▁▁</td></tr><tr><td>eval/runtime</td><td>▁▆▄▃█▅▄</td></tr><tr><td>eval/samples_per_second</td><td>█▃▅▆▁▄▅</td></tr><tr><td>eval/steps_per_second</td><td>█▃▅▆▁▄▅</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▄▃▂▁▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.59222</td></tr><tr><td>eval/loss</td><td>1.55393</td></tr><tr><td>eval/runtime</td><td>30.4683</td></tr><tr><td>eval/samples_per_second</td><td>1220.941</td></tr><tr><td>eval/steps_per_second</td><td>76.309</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>21700</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>1.2974</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>1.53744</td></tr><tr><td>train/train_runtime</td><td>3122.942</td></tr><tr><td>train/train_samples_per_second</td><td>333.532</td></tr><tr><td>train/train_steps_per_second</td><td>6.949</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">trim-flower-106</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/bwifcff0' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/bwifcff0</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230615_202542-bwifcff0\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230615_211745-dpwuarai</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/dpwuarai' target=\"_blank\">kind-butterfly-107</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/dpwuarai' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/dpwuarai</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16275' max='16275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16275/16275 50:27, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>1.432500</td>\n",
              "      <td>1.636760</td>\n",
              "      <td>0.576789</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>1.210100</td>\n",
              "      <td>1.541181</td>\n",
              "      <td>0.602562</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.993100</td>\n",
              "      <td>1.455965</td>\n",
              "      <td>0.638115</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.813100</td>\n",
              "      <td>1.465411</td>\n",
              "      <td>0.650149</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.661700</td>\n",
              "      <td>1.456136</td>\n",
              "      <td>0.663546</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.542800</td>\n",
              "      <td>1.464734</td>\n",
              "      <td>0.673369</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.455400</td>\n",
              "      <td>1.479500</td>\n",
              "      <td>0.675929</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-15 22:08:20,439]\u001b[0m Trial 3 finished with value: 0.6759294417959156 and parameters: {'learning_rate': 3.874103388061133e-05, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 4, 'warmup_steps': 241, 'weight_decay': 0.03309433065761276, 'per_device_eval_batch_size': 16}. Best is trial 3 with value: 0.6759294417959156.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "32eaa37f3f41414fa129b366bd9e4d60",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▅▆▇██</td></tr><tr><td>eval/loss</td><td>█▄▁▁▁▁▂</td></tr><tr><td>eval/runtime</td><td>▁▇▃▄▅▂█</td></tr><tr><td>eval/samples_per_second</td><td>█▂▆▅▄▇▁</td></tr><tr><td>eval/steps_per_second</td><td>█▂▆▅▄▇▁</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▆▅▄▂▂▁</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.67593</td></tr><tr><td>eval/loss</td><td>1.4795</td></tr><tr><td>eval/runtime</td><td>30.7146</td></tr><tr><td>eval/samples_per_second</td><td>1211.152</td></tr><tr><td>eval/steps_per_second</td><td>75.697</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>16275</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.4554</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.87268</td></tr><tr><td>train/train_runtime</td><td>3040.5351</td></tr><tr><td>train/train_samples_per_second</td><td>342.571</td></tr><tr><td>train/train_steps_per_second</td><td>5.353</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">kind-butterfly-107</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/dpwuarai' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/dpwuarai</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230615_211745-dpwuarai\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ce57f54344d9421da2cfa934d9eb58d8",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230615_220825-8lpwbxrb</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/8lpwbxrb' target=\"_blank\">youthful-morning-108</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/8lpwbxrb' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/8lpwbxrb</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16275' max='16275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16275/16275 50:34, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.326400</td>\n",
              "      <td>1.553888</td>\n",
              "      <td>0.678504</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.246400</td>\n",
              "      <td>1.640012</td>\n",
              "      <td>0.675519</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.180500</td>\n",
              "      <td>1.707828</td>\n",
              "      <td>0.677402</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.135500</td>\n",
              "      <td>1.781306</td>\n",
              "      <td>0.677316</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.110500</td>\n",
              "      <td>1.834053</td>\n",
              "      <td>0.682261</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.106100</td>\n",
              "      <td>1.863115</td>\n",
              "      <td>0.683826</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.158800</td>\n",
              "      <td>1.871103</td>\n",
              "      <td>0.682751</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-15 22:59:09,386]\u001b[0m Trial 4 finished with value: 0.6827507978534815 and parameters: {'learning_rate': 1.2388692006618435e-05, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 4, 'warmup_steps': 253, 'weight_decay': 0.06542097187746301, 'per_device_eval_batch_size': 16}. Best is trial 4 with value: 0.6827507978534815.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▄▁▃▃▇█▇</td></tr><tr><td>eval/loss</td><td>▁▃▄▆▇██</td></tr><tr><td>eval/runtime</td><td>▆▁█▅▅▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▃█▁▄▄▄█</td></tr><tr><td>eval/steps_per_second</td><td>▃█▁▄▄▄█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>█▅▃▂▁▁▃</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68275</td></tr><tr><td>eval/loss</td><td>1.8711</td></tr><tr><td>eval/runtime</td><td>30.3733</td></tr><tr><td>eval/samples_per_second</td><td>1224.762</td></tr><tr><td>eval/steps_per_second</td><td>76.548</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>16275</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1588</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.1806</td></tr><tr><td>train/train_runtime</td><td>3048.9394</td></tr><tr><td>train/train_samples_per_second</td><td>341.627</td></tr><tr><td>train/train_steps_per_second</td><td>5.338</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">youthful-morning-108</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/8lpwbxrb' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/8lpwbxrb</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230615_220825-8lpwbxrb\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "db4f92eaab564785b72e01ab2013a3ba",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016916666666656966, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230615_225915-gayno701</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/gayno701' target=\"_blank\">jumping-butterfly-109</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/gayno701' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/gayno701</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13020' max='13020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13020/13020 49:45, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.085300</td>\n",
              "      <td>2.097570</td>\n",
              "      <td>0.671165</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.076700</td>\n",
              "      <td>2.144343</td>\n",
              "      <td>0.675061</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.061000</td>\n",
              "      <td>2.258623</td>\n",
              "      <td>0.673073</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.047300</td>\n",
              "      <td>2.325688</td>\n",
              "      <td>0.678121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.041000</td>\n",
              "      <td>2.364275</td>\n",
              "      <td>0.680753</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.045300</td>\n",
              "      <td>2.390846</td>\n",
              "      <td>0.680737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.105200</td>\n",
              "      <td>2.375340</td>\n",
              "      <td>0.682636</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-15 23:49:10,172]\u001b[0m Trial 5 finished with value: 0.6826355819059462 and parameters: {'learning_rate': 2.080479240207874e-05, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 5, 'warmup_steps': 199, 'weight_decay': 0.01846062685291469, 'per_device_eval_batch_size': 16}. Best is trial 4 with value: 0.6827507978534815.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▁▃▂▅▇▇█</td></tr><tr><td>eval/loss</td><td>▁▂▅▆▇██</td></tr><tr><td>eval/runtime</td><td>██▇▆▄▅▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▁▂▃▅▄█</td></tr><tr><td>eval/steps_per_second</td><td>▁▁▂▃▅▄█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>▆▅▃▂▁▁█</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68264</td></tr><tr><td>eval/loss</td><td>2.37534</td></tr><tr><td>eval/runtime</td><td>30.3368</td></tr><tr><td>eval/samples_per_second</td><td>1226.233</td></tr><tr><td>eval/steps_per_second</td><td>76.64</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>13020</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.1052</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.06596</td></tr><tr><td>train/train_runtime</td><td>3000.7649</td></tr><tr><td>train/train_samples_per_second</td><td>347.111</td></tr><tr><td>train/train_steps_per_second</td><td>4.339</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">jumping-butterfly-109</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/gayno701' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/gayno701</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230615_225915-gayno701\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230615_234915-48uem0vc</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/48uem0vc' target=\"_blank\">deft-vortex-110</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/48uem0vc' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/48uem0vc</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='13020' max='13020' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [13020/13020 49:39, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.008700</td>\n",
              "      <td>2.445317</td>\n",
              "      <td>0.684123</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.006100</td>\n",
              "      <td>2.508686</td>\n",
              "      <td>0.685239</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.003800</td>\n",
              "      <td>2.544679</td>\n",
              "      <td>0.683121</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.003200</td>\n",
              "      <td>2.577409</td>\n",
              "      <td>0.686150</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.002700</td>\n",
              "      <td>2.605425</td>\n",
              "      <td>0.684594</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.003700</td>\n",
              "      <td>2.626765</td>\n",
              "      <td>0.684006</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.046800</td>\n",
              "      <td>2.619718</td>\n",
              "      <td>0.683562</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-16 00:39:03,685]\u001b[0m Trial 6 finished with value: 0.6835624435402132 and parameters: {'learning_rate': 3.3600626494738905e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 5, 'warmup_steps': 21, 'weight_decay': 0.048934306251516936, 'per_device_eval_batch_size': 16}. Best is trial 6 with value: 0.6835624435402132.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "ff6127c406ab4e9295f38101f7ced412",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▃▆▁█▄▃▂</td></tr><tr><td>eval/loss</td><td>▁▃▅▆▇██</td></tr><tr><td>eval/runtime</td><td>█▆▅▇▇█▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▃▄▂▂▁█</td></tr><tr><td>eval/steps_per_second</td><td>▁▃▄▂▂▁█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▄▃▂▁</td></tr><tr><td>train/loss</td><td>▂▂▁▁▁▁█</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68356</td></tr><tr><td>eval/loss</td><td>2.61972</td></tr><tr><td>eval/runtime</td><td>30.3474</td></tr><tr><td>eval/samples_per_second</td><td>1225.805</td></tr><tr><td>eval/steps_per_second</td><td>76.613</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>13020</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0468</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.01072</td></tr><tr><td>train/train_runtime</td><td>2993.4919</td></tr><tr><td>train/train_samples_per_second</td><td>347.955</td></tr><tr><td>train/train_steps_per_second</td><td>4.349</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">deft-vortex-110</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/48uem0vc' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/48uem0vc</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230615_234915-48uem0vc\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "a5b5e406e7234ec592ec527afc58ba0d",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016933333333327028, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230616_003909-x9d8z84l</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/x9d8z84l' target=\"_blank\">clean-energy-111</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/x9d8z84l' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/x9d8z84l</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='16275' max='16275' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [16275/16275 56:35, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.001600</td>\n",
              "      <td>2.767132</td>\n",
              "      <td>0.683031</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.001700</td>\n",
              "      <td>2.851578</td>\n",
              "      <td>0.682564</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.001200</td>\n",
              "      <td>2.916034</td>\n",
              "      <td>0.680460</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.001000</td>\n",
              "      <td>2.954668</td>\n",
              "      <td>0.683656</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000800</td>\n",
              "      <td>3.005400</td>\n",
              "      <td>0.682499</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.001800</td>\n",
              "      <td>3.017778</td>\n",
              "      <td>0.682107</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.046000</td>\n",
              "      <td>3.006298</td>\n",
              "      <td>0.681568</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-16 01:35:53,713]\u001b[0m Trial 7 finished with value: 0.681568347118281 and parameters: {'learning_rate': 6.157357545353556e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 4, 'warmup_steps': 202, 'weight_decay': 0.054449053387397736, 'per_device_eval_batch_size': 16}. Best is trial 6 with value: 0.6835624435402132.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "f6fa0316e8b943af8b3e34a45ce364e0",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>▇▆▁█▅▅▃</td></tr><tr><td>eval/loss</td><td>▁▃▅▆███</td></tr><tr><td>eval/runtime</td><td>▁▁███▇▇</td></tr><tr><td>eval/samples_per_second</td><td>██▁▁▁▂▂</td></tr><tr><td>eval/steps_per_second</td><td>██▁▁▁▂▂</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>▁▁▁▁▁▁█</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68157</td></tr><tr><td>eval/loss</td><td>3.0063</td></tr><tr><td>eval/runtime</td><td>36.6567</td></tr><tr><td>eval/samples_per_second</td><td>1014.82</td></tr><tr><td>eval/steps_per_second</td><td>63.426</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>16275</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.046</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.00773</td></tr><tr><td>train/train_runtime</td><td>3410.0179</td></tr><tr><td>train/train_samples_per_second</td><td>305.453</td></tr><tr><td>train/train_steps_per_second</td><td>4.773</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">clean-energy-111</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/x9d8z84l' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/x9d8z84l</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230616_003909-x9d8z84l\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "33aa3ad51e6849f2a4da1d051dd4c74b",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230616_013559-m6sswbom</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/m6sswbom' target=\"_blank\">earnest-lion-112</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/m6sswbom' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/m6sswbom</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='21700' max='21700' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [21700/21700 1:03:32, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000500</td>\n",
              "      <td>3.078765</td>\n",
              "      <td>0.685397</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000400</td>\n",
              "      <td>3.131454</td>\n",
              "      <td>0.683675</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>3.140082</td>\n",
              "      <td>0.685281</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>3.173237</td>\n",
              "      <td>0.684737</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000200</td>\n",
              "      <td>3.202909</td>\n",
              "      <td>0.684679</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000300</td>\n",
              "      <td>3.209505</td>\n",
              "      <td>0.682769</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.038300</td>\n",
              "      <td>3.205823</td>\n",
              "      <td>0.682474</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-16 02:39:40,225]\u001b[0m Trial 8 finished with value: 0.6824744700781978 and parameters: {'learning_rate': 3.0654403316623304e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 3, 'warmup_steps': 127, 'weight_decay': 0.08548994314004767, 'per_device_eval_batch_size': 16}. Best is trial 6 with value: 0.6835624435402132.\u001b[0m\n",
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "6b3936a8707c47bc8043b15f8b017576",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.001 MB of 0.001 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>█▄█▆▆▂▁</td></tr><tr><td>eval/loss</td><td>▁▄▄▆███</td></tr><tr><td>eval/runtime</td><td>█▆▁▄▆▇▁</td></tr><tr><td>eval/samples_per_second</td><td>▁▃█▅▃▂█</td></tr><tr><td>eval/steps_per_second</td><td>▁▃█▅▃▂█</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>▁▁▁▁▁▁█</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68247</td></tr><tr><td>eval/loss</td><td>3.20582</td></tr><tr><td>eval/runtime</td><td>36.725</td></tr><tr><td>eval/samples_per_second</td><td>1012.935</td></tr><tr><td>eval/steps_per_second</td><td>63.308</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>21700</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.0383</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.00575</td></tr><tr><td>train/train_runtime</td><td>3826.4964</td></tr><tr><td>train/train_samples_per_second</td><td>272.207</td></tr><tr><td>train/train_steps_per_second</td><td>5.671</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">earnest-lion-112</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/m6sswbom' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/m6sswbom</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230616_013559-m6sswbom\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "502473480a734dda857fdcb1f207d503",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.01693333333338766, max=1.0)…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230616_023946-covz0eqf</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/covz0eqf' target=\"_blank\">blooming-aardvark-113</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/covz0eqf' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/covz0eqf</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='32550' max='32550' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [32550/32550 1:09:07, Epoch 7/7]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>3.257234</td>\n",
              "      <td>0.686035</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>3.297982</td>\n",
              "      <td>0.686063</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.319265</td>\n",
              "      <td>0.685849</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.345392</td>\n",
              "      <td>0.685508</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.000000</td>\n",
              "      <td>3.381916</td>\n",
              "      <td>0.684831</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.000100</td>\n",
              "      <td>3.388909</td>\n",
              "      <td>0.683458</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.034000</td>\n",
              "      <td>3.381931</td>\n",
              "      <td>0.682824</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[32m[I 2023-06-16 03:49:02,896]\u001b[0m Trial 9 finished with value: 0.6828236101795916 and parameters: {'learning_rate': 1.093274712731568e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 2, 'warmup_steps': 140, 'weight_decay': 0.07056041638210678, 'per_device_eval_batch_size': 16}. Best is trial 6 with value: 0.6835624435402132.\u001b[0m\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>███▇▅▂▁</td></tr><tr><td>eval/loss</td><td>▁▃▄▆███</td></tr><tr><td>eval/runtime</td><td>█▃▇▂▆▁▇</td></tr><tr><td>eval/samples_per_second</td><td>▁▆▂▇▃█▂</td></tr><tr><td>eval/steps_per_second</td><td>▁▆▂▇▃█▂</td></tr><tr><td>train/epoch</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/global_step</td><td>▁▁▂▂▃▃▅▅▆▆▇▇███</td></tr><tr><td>train/learning_rate</td><td>█▇▆▅▃▂▁</td></tr><tr><td>train/loss</td><td>▁▁▁▁▁▁█</td></tr><tr><td>train/total_flos</td><td>▁</td></tr><tr><td>train/train_loss</td><td>▁</td></tr><tr><td>train/train_runtime</td><td>▁</td></tr><tr><td>train/train_samples_per_second</td><td>▁</td></tr><tr><td>train/train_steps_per_second</td><td>▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>eval/f1</td><td>0.68282</td></tr><tr><td>eval/loss</td><td>3.38193</td></tr><tr><td>eval/runtime</td><td>37.159</td></tr><tr><td>eval/samples_per_second</td><td>1001.104</td></tr><tr><td>eval/steps_per_second</td><td>62.569</td></tr><tr><td>train/epoch</td><td>7.0</td></tr><tr><td>train/global_step</td><td>32550</td></tr><tr><td>train/learning_rate</td><td>0.0</td></tr><tr><td>train/loss</td><td>0.034</td></tr><tr><td>train/total_flos</td><td>3.4275514226688e+16</td></tr><tr><td>train/train_loss</td><td>0.00489</td></tr><tr><td>train/train_runtime</td><td>4162.6542</td></tr><tr><td>train/train_samples_per_second</td><td>250.225</td></tr><tr><td>train/train_steps_per_second</td><td>7.82</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">blooming-aardvark-113</strong> at: <a href='https://wandb.ai/delta_lxr/huggingface/runs/covz0eqf' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/covz0eqf</a><br/>Synced 6 W&B file(s), 0 media file(s), 0 artifact file(s) and 0 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>.\\wandb\\run-20230616_023946-covz0eqf\\logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "roberta-base chosen hyperparameters:\n",
            "{'learning_rate': 3.3600626494738905e-06, 'num_train_epochs': 7, 'seed': 9, 'per_device_train_batch_size': 16, 'gradient_accumulation_steps': 5, 'warmup_steps': 21, 'weight_decay': 0.048934306251516936, 'per_device_eval_batch_size': 16}\n"
          ]
        }
      ],
      "source": [
        "roberta.hyper_parameters_search(train_args)"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "2a042843",
      "metadata": {
        "id": "2a042843"
      },
      "source": [
        "## Model 2 Train on best hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "57ddd90d",
      "metadata": {
        "colab": {
          "referenced_widgets": [
            "1a211ad1ba5e4572b34f0c845c606685"
          ]
        },
        "id": "57ddd90d",
        "outputId": "93793c63-3403-4159-8ee5-332b69d5b31a"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "C:\\Users\\liyag\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\optimization.py:391: FutureWarning: This implementation of AdamW is deprecated and will be removed in a future version. Use the PyTorch implementation torch.optim.AdamW instead, or set `no_deprecation_warning=True` to disable this warning\n",
            "  warnings.warn(\n"
          ]
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "1a211ad1ba5e4572b34f0c845c606685",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='Waiting for wandb.init()...\\r'), FloatProgress(value=0.016666666666666666, max=1.0…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "wandb version 0.15.4 is available!  To upgrade, please run:\n",
              " $ pip install wandb --upgrade"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.15.2"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>C:\\Users\\liyag\\OneDrive - mail.tau.ac.il\\Desktop\\NLP\\Jonathan\\wandb\\run-20230616_034908-1u4xnun0</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/delta_lxr/huggingface/runs/1u4xnun0' target=\"_blank\">gallant-fog-114</a></strong> to <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/delta_lxr/huggingface' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/delta_lxr/huggingface/runs/1u4xnun0' target=\"_blank\">https://wandb.ai/delta_lxr/huggingface/runs/1u4xnun0</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "\n",
              "    <div>\n",
              "      \n",
              "      <progress value='74500' max='186000' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
              "      [ 74500/186000 1:42:37 < 2:33:35, 12.10 it/s, Epoch 8.01/20]\n",
              "    </div>\n",
              "    <table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              " <tr style=\"text-align: left;\">\n",
              "      <th>Epoch</th>\n",
              "      <th>Training Loss</th>\n",
              "      <th>Validation Loss</th>\n",
              "      <th>F1</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <td>1</td>\n",
              "      <td>0.011700</td>\n",
              "      <td>3.663433</td>\n",
              "      <td>0.676860</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>2</td>\n",
              "      <td>0.021200</td>\n",
              "      <td>3.769520</td>\n",
              "      <td>0.674983</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>3</td>\n",
              "      <td>0.028300</td>\n",
              "      <td>3.862758</td>\n",
              "      <td>0.673112</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>4</td>\n",
              "      <td>0.035900</td>\n",
              "      <td>3.957393</td>\n",
              "      <td>0.676091</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>5</td>\n",
              "      <td>0.049400</td>\n",
              "      <td>4.000326</td>\n",
              "      <td>0.676022</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>6</td>\n",
              "      <td>0.072000</td>\n",
              "      <td>3.980752</td>\n",
              "      <td>0.673859</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>7</td>\n",
              "      <td>0.184800</td>\n",
              "      <td>3.727416</td>\n",
              "      <td>0.674466</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <td>8</td>\n",
              "      <td>0.282000</td>\n",
              "      <td>3.490899</td>\n",
              "      <td>0.676872</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table><p>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "ename": "RuntimeError",
          "evalue": "[enforce fail at C:\\cb\\pytorch_1000000000000\\work\\caffe2\\serialize\\inline_container.cc:337] . unexpected pos 860796096 vs 860795984",
          "output_type": "error",
          "traceback": [
            "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\serialization.py:441\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    440\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[1;32m--> 441\u001b[0m     \u001b[43m_save\u001b[49m\u001b[43m(\u001b[49m\u001b[43mobj\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mopened_zipfile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_module\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mpickle_protocol\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    442\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\serialization.py:668\u001b[0m, in \u001b[0;36m_save\u001b[1;34m(obj, zip_file, pickle_module, pickle_protocol)\u001b[0m\n\u001b[0;32m    667\u001b[0m num_bytes \u001b[38;5;241m=\u001b[39m storage\u001b[38;5;241m.\u001b[39mnbytes()\n\u001b[1;32m--> 668\u001b[0m \u001b[43mzip_file\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_record\u001b[49m\u001b[43m(\u001b[49m\u001b[43mname\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstorage\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata_ptr\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_bytes\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\caffe2\\serialize\\inline_container.cc:476] . PytorchStreamWriter failed writing file data/74: file write failed",
            "\nDuring handling of the above exception, another exception occurred:\n",
            "\u001b[1;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
            "Cell \u001b[1;32mIn[17], line 32\u001b[0m\n\u001b[0;32m      1\u001b[0m best_args \u001b[38;5;241m=\u001b[39m TrainingArguments(output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m      2\u001b[0m                          overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m      3\u001b[0m                          per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     14\u001b[0m                          warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m188\u001b[39m,\n\u001b[0;32m     15\u001b[0m                          report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[0;32m     17\u001b[0m eval_args \u001b[38;5;241m=\u001b[39m TrainingArguments(output_dir\u001b[38;5;241m=\u001b[39moutput_dir,\n\u001b[0;32m     18\u001b[0m                          overwrite_output_dir\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[0;32m     19\u001b[0m                          per_device_train_batch_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m16\u001b[39m,\n\u001b[1;32m   (...)\u001b[0m\n\u001b[0;32m     29\u001b[0m                          warmup_steps\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m188\u001b[39m,\n\u001b[0;32m     30\u001b[0m                          report_to\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mwandb\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m---> 32\u001b[0m \u001b[43mroberta\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43mbest_args\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m     34\u001b[0m \u001b[38;5;66;03m# save model's state dict\u001b[39;00m\n\u001b[0;32m     35\u001b[0m roberta\u001b[38;5;241m.\u001b[39msave_model(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mroberta_regular\u001b[39m\u001b[38;5;124m'\u001b[39m)\n",
            "Cell \u001b[1;32mIn[13], line 28\u001b[0m, in \u001b[0;36mModel.train\u001b[1;34m(self, train_args)\u001b[0m\n\u001b[0;32m     20\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtrain\u001b[39m(\u001b[38;5;28mself\u001b[39m, train_args):\n\u001b[0;32m     21\u001b[0m     trainer \u001b[38;5;241m=\u001b[39m Trainer(\n\u001b[0;32m     22\u001b[0m         model\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel,\n\u001b[0;32m     23\u001b[0m         args\u001b[38;5;241m=\u001b[39mtrain_args,\n\u001b[0;32m     24\u001b[0m         train_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtrain\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     25\u001b[0m         eval_dataset\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mtokenized_dataset[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mtest\u001b[39m\u001b[38;5;124m'\u001b[39m],\n\u001b[0;32m     26\u001b[0m         compute_metrics\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetric_fn)\n\u001b[1;32m---> 28\u001b[0m     \u001b[43mtrainer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtrain\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:1662\u001b[0m, in \u001b[0;36mTrainer.train\u001b[1;34m(self, resume_from_checkpoint, trial, ignore_keys_for_eval, **kwargs)\u001b[0m\n\u001b[0;32m   1657\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel_wrapped \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmodel\n\u001b[0;32m   1659\u001b[0m inner_training_loop \u001b[38;5;241m=\u001b[39m find_executable_batch_size(\n\u001b[0;32m   1660\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_inner_training_loop, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_train_batch_size, args\u001b[38;5;241m.\u001b[39mauto_find_batch_size\n\u001b[0;32m   1661\u001b[0m )\n\u001b[1;32m-> 1662\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43minner_training_loop\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1663\u001b[0m \u001b[43m    \u001b[49m\u001b[43margs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1664\u001b[0m \u001b[43m    \u001b[49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mresume_from_checkpoint\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1665\u001b[0m \u001b[43m    \u001b[49m\u001b[43mtrial\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1666\u001b[0m \u001b[43m    \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m,\u001b[49m\n\u001b[0;32m   1667\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2006\u001b[0m, in \u001b[0;36mTrainer._inner_training_loop\u001b[1;34m(self, batch_size, args, resume_from_checkpoint, trial, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2003\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mepoch \u001b[38;5;241m=\u001b[39m epoch \u001b[38;5;241m+\u001b[39m (step \u001b[38;5;241m+\u001b[39m \u001b[38;5;241m1\u001b[39m \u001b[38;5;241m+\u001b[39m steps_skipped) \u001b[38;5;241m/\u001b[39m steps_in_epoch\n\u001b[0;32m   2004\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_step_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n\u001b[1;32m-> 2006\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_maybe_log_save_evaluate\u001b[49m\u001b[43m(\u001b[49m\u001b[43mtr_loss\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepoch\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mignore_keys_for_eval\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2007\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m   2008\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_substep_end(args, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2291\u001b[0m, in \u001b[0;36mTrainer._maybe_log_save_evaluate\u001b[1;34m(self, tr_loss, model, trial, epoch, ignore_keys_for_eval)\u001b[0m\n\u001b[0;32m   2288\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_report_to_hp_search(trial, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate\u001b[38;5;241m.\u001b[39mglobal_step, metrics)\n\u001b[0;32m   2290\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol\u001b[38;5;241m.\u001b[39mshould_save:\n\u001b[1;32m-> 2291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_save_checkpoint\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtrial\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmetrics\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmetrics\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2292\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcallback_handler\u001b[38;5;241m.\u001b[39mon_save(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstate, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcontrol)\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\transformers\\trainer.py:2382\u001b[0m, in \u001b[0;36mTrainer._save_checkpoint\u001b[1;34m(self, model, trial, metrics)\u001b[0m\n\u001b[0;32m   2379\u001b[0m             torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mscaler\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, SCALER_NAME))\n\u001b[0;32m   2380\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39margs\u001b[38;5;241m.\u001b[39mshould_save \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdeepspeed:\n\u001b[0;32m   2381\u001b[0m     \u001b[38;5;66;03m# deepspeed.save_checkpoint above saves model/optim/sched\u001b[39;00m\n\u001b[1;32m-> 2382\u001b[0m     \u001b[43mtorch\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msave\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43moptimizer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstate_dict\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mos\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mjoin\u001b[49m\u001b[43m(\u001b[49m\u001b[43moutput_dir\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mOPTIMIZER_NAME\u001b[49m\u001b[43m)\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   2383\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m warnings\u001b[38;5;241m.\u001b[39mcatch_warnings(record\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m) \u001b[38;5;28;01mas\u001b[39;00m caught_warnings:\n\u001b[0;32m   2384\u001b[0m         torch\u001b[38;5;241m.\u001b[39msave(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlr_scheduler\u001b[38;5;241m.\u001b[39mstate_dict(), os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mjoin(output_dir, SCHEDULER_NAME))\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\serialization.py:440\u001b[0m, in \u001b[0;36msave\u001b[1;34m(obj, f, pickle_module, pickle_protocol, _use_new_zipfile_serialization)\u001b[0m\n\u001b[0;32m    437\u001b[0m _check_save_filelike(f)\n\u001b[0;32m    439\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m _use_new_zipfile_serialization:\n\u001b[1;32m--> 440\u001b[0m     \u001b[38;5;28;01mwith\u001b[39;00m _open_zipfile_writer(f) \u001b[38;5;28;01mas\u001b[39;00m opened_zipfile:\n\u001b[0;32m    441\u001b[0m         _save(obj, opened_zipfile, pickle_module, pickle_protocol)\n\u001b[0;32m    442\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m\n",
            "File \u001b[1;32m~\\AppData\\Local\\anaconda3\\lib\\site-packages\\torch\\serialization.py:291\u001b[0m, in \u001b[0;36m_open_zipfile_writer_file.__exit__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m    290\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m__exit__\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;241m*\u001b[39margs) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m--> 291\u001b[0m     \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfile_like\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_end_of_file\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
            "\u001b[1;31mRuntimeError\u001b[0m: [enforce fail at C:\\cb\\pytorch_1000000000000\\work\\caffe2\\serialize\\inline_container.cc:337] . unexpected pos 860796096 vs 860795984"
          ]
        }
      ],
      "source": [
        "best_args = TrainingArguments(output_dir=output_dir,\n",
        "                         overwrite_output_dir=True,\n",
        "                         per_device_train_batch_size=16,\n",
        "                         per_device_eval_batch_size=16,\n",
        "                         seed = 9,\n",
        "                         learning_rate=7.600882468235104e-06,\n",
        "                         weight_decay=0.0511022637864972,\n",
        "                         greater_is_better=True,\n",
        "                         evaluation_strategy='epoch',\n",
        "                         do_train=True,\n",
        "                         num_train_epochs=20,\n",
        "                         gradient_accumulation_steps=1,\n",
        "                         logging_strategy='epoch',\n",
        "                         warmup_steps=188,\n",
        "                         report_to='wandb')\n",
        "\n",
        "eval_args = TrainingArguments(output_dir=output_dir,\n",
        "                         overwrite_output_dir=True,\n",
        "                         per_device_train_batch_size=16,\n",
        "                         per_device_eval_batch_size=16,\n",
        "                          seed = 9,\n",
        "                         learning_rate=7.600882468235104e-06,\n",
        "                         weight_decay=0.0511022637864972,\n",
        "                         greater_is_better=True,\n",
        "                         evaluation_strategy='epoch',\n",
        "                         do_train=False,\n",
        "                         gradient_accumulation_steps=1,\n",
        "                         logging_strategy='epoch',\n",
        "                         warmup_steps=188,\n",
        "                         report_to='wandb')\n",
        "\n",
        "roberta.train(best_args)\n",
        "\n",
        "# save model's state dict\n",
        "roberta.save_model('roberta_regular')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "edc3d73e",
      "metadata": {
        "id": "edc3d73e"
      },
      "source": [
        "## Best model summary:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "7a5a8dff",
      "metadata": {
        "id": "7a5a8dff",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "roberta.model.load_state_dict(torch.load(Path(home_dir, 'best_roberta.py')))\n",
        "roberta.evaluate(eval_args)\n",
        "roberta.print_model_size()\n",
        "roberta.print_model()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "9df52b8e",
      "metadata": {
        "id": "9df52b8e"
      },
      "source": [
        "## a. Model 2 Pruning"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4934b902",
      "metadata": {
        "id": "4934b902",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "pruned_roberta = Model(model_name_2, datasets, num_of_classes)\n",
        "pruned_roberta.tokenize(token_args)\n",
        "\n",
        "# Load the model's best parameters\n",
        "pruned_roberta.model.load_state_dict(torch.load(Path(home_dir, 'roberta_regular.py')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "dd17460d",
      "metadata": {
        "id": "dd17460d",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "# Prune and train one epoch\n",
        "pruned_bert.pruning(0.2)\n",
        "train_one_epoch_args = TrainingArguments(output_dir=output_dir,\n",
        "                             overwrite_output_dir=True,\n",
        "                             greater_is_better=True,\n",
        "                             evaluation_strategy='epoch',\n",
        "                             do_train=True,\n",
        "                             logging_strategy='epoch',\n",
        "                             num_train_epochs=1,\n",
        "                             save_strategy='no')\n",
        "\n",
        "pruned_roberta.train(train_one_epoch_args)\n",
        "pruned_roberta.save_model('roberta_pruning')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4344f8ca",
      "metadata": {
        "id": "4344f8ca"
      },
      "outputs": [],
      "source": [
        "# Pruned model summary\n",
        "pruned_roberta.evaluate(eval_args)\n",
        "pruned_roberta.print_model_size()\n",
        "pruned_roberta.print_model()"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "nqgsJl01J14x",
      "metadata": {
        "id": "nqgsJl01J14x"
      },
      "source": [
        "##b.Model 2 Quantization"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "LnYh3IjyKAxD",
      "metadata": {
        "id": "LnYh3IjyKAxD"
      },
      "outputs": [],
      "source": [
        "# Load the quantized model\n",
        "\n",
        "quantization_bert = Model(model_name_1, datasets, num_of_classes)\n",
        "quantization_bert.tokenize(token_args)\n",
        "quantization_bert.model.load_state_dict(torch.load(Path(home_dir, 'roberta_regular.pt')))\n",
        "\n",
        "# Train the quantized model for additional epochs\n",
        "train_args = TrainingArguments(\n",
        "    output_dir=output_dir,\n",
        "    overwrite_output_dir=True,\n",
        "    evaluation_strategy='epoch',\n",
        "    logging_strategy='epoch',\n",
        "    num_train_epochs=5,  # Specify the desired number of additional epochs\n",
        "    save_strategy='no'\n",
        ")\n",
        "\n",
        "quantization_bert.train(train_args)\n",
        "\n",
        "# Save the trained quantized model's state dict\n",
        "quantization_bert.save_model('quantized_reberta.pt')"
      ]
    },
    {
      "attachments": {},
      "cell_type": "markdown",
      "id": "74079452",
      "metadata": {
        "id": "74079452"
      },
      "source": [
        "## c. Model 2 Knowledge-Distillation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "4f86c8de",
      "metadata": {
        "id": "4f86c8de"
      },
      "outputs": [],
      "source": [
        "hidden_dim = 128\n",
        "num_layers = 2\n",
        "num_heads = 2\n",
        "num_classes = 62\n",
        "epochs = 1\n",
        "alpha_teacher = 0.8\n",
        "lr=0.001\n",
        "loss_func = nn.MSELoss()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "50ffe072",
      "metadata": {
        "id": "50ffe072",
        "scrolled": true
      },
      "outputs": [],
      "source": [
        "kl_roberta = Model(model_name_2, datasets, num_of_classes)\n",
        "kl_roberta.tokenize(token_args)\n",
        "\n",
        "# Load the model's best parameters\n",
        "kl_roberta.model.load_state_dict(torch.load(Path(home_dir, 'best_roberta.py')))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "f999194f",
      "metadata": {
        "id": "f999194f"
      },
      "outputs": [],
      "source": [
        "roberta_student = kl_roberta.kl(hidden_dim,num_layers,num_heads,epochs, alpha_teacher, lr, loss_func)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "455ae743",
      "metadata": {
        "id": "455ae743"
      },
      "outputs": [],
      "source": [
        "roberta_student_model = Model(model_name_1, datasets, num_of_classes)\n",
        "roberta_student_model.tokenize(token_args)\n",
        "roberta_student_model.model = roberta_student\n",
        "\n",
        "roberta_student_model.save_model('roberta_student')"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "16929406",
      "metadata": {
        "id": "16929406"
      },
      "outputs": [],
      "source": [
        "# Student model summary\n",
        "roberta_student_model.evaluate(eval_args)\n",
        "roberta_student_model.print_model_size()\n",
        "roberta_student_model.print_model()"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "display_name": "venv",
      "language": "python",
      "name": "venv"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.5"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
